# DSL Architecture - Proper Separation of Responsibilities

**Version**: 1.0  
**Date**: 2025-01-27  
**Status**: Architecture Correction  

## ðŸŽ¯ **The Two Distinct DSL Responsibilities**

### **1. LLM Response Structure Discovery DSL**
**Purpose**: Understand and parse diverse LLM provider response formats
**Input**: Raw JSON from any LLM provider (OpenAI, Anthropic, Gemini, etc.)
**Output**: Structured field classifications and extracted data

### **2. Semantic Convention Mapping DSL** 
**Purpose**: Convert between different observability framework conventions
**Input**: Structured data from discovery + source convention type
**Output**: Target convention format (OpenInference, Traceloop, OpenLit, HoneyHive)

## ðŸ—ï¸ **Corrected DSL Architecture**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Raw LLM Provider Response                â”‚
â”‚  OpenAI JSON â”‚ Anthropic JSON â”‚ Gemini JSON â”‚ Unknown JSON  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            LLM Response Structure Discovery DSL             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚Structure Patternâ”‚  â”‚Field Classifier â”‚  â”‚Content Extractâ”‚ â”‚
â”‚  â”‚   Recognition   â”‚  â”‚      DSL        â”‚  â”‚     DSL      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Normalized Field Data                       â”‚
â”‚     {field_path, field_type, content, confidence}          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             Semantic Convention Mapping DSL                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ OpenInference   â”‚  â”‚   Traceloop     â”‚  â”‚  HoneyHive   â”‚ â”‚
â”‚  â”‚   Mapping       â”‚  â”‚   Mapping       â”‚  â”‚   Mapping    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Target Convention Output                       â”‚
â”‚   OpenInference â”‚ Traceloop â”‚ OpenLit â”‚ HoneyHive Schema   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ“‹ **DSL Component Breakdown**

### **Component 1: LLM Response Structure Discovery DSL**

#### **1.1 Structure Pattern Recognition DSL**
```yaml
# llm_response_discovery/structure_patterns.yaml
structure_patterns:
  version: "1.0"
  
  # Detect provider response patterns
  provider_signatures:
    openai_pattern:
      required_paths: ["choices", "usage", "model"]
      optional_paths: ["id", "created", "system_fingerprint"]
      structure_type: "flat_choices_array"
      
    anthropic_pattern:
      required_paths: ["content", "usage", "model"]
      optional_paths: ["id", "type", "stop_reason"]
      structure_type: "content_array"
      
    gemini_pattern:
      required_paths: ["candidates", "usageMetadata"]
      optional_paths: ["safetyRatings", "modelVersion"]
      structure_type: "candidates_array"
  
  # Structure navigation rules
  navigation_rules:
    message_content_paths:
      - pattern: "choices.*.message.content"
        provider: "openai"
        confidence: 0.95
      - pattern: "content.*.text"
        provider: "anthropic" 
        confidence: 0.90
      - pattern: "candidates.*.content.parts.*.text"
        provider: "gemini"
        confidence: 0.85
```

#### **1.2 Field Classification DSL**
```yaml
# llm_response_discovery/field_classification.yaml
field_classification:
  version: "1.0"
  
  # Semantic field type classification
  field_types:
    message_content:
      indicators:
        path_patterns: ["message.content", "content.text", "parts.text"]
        content_patterns: ["string", "length > 0"]
        context_clues: ["role", "assistant", "user"]
      
    token_usage:
      indicators:
        path_patterns: ["usage", "tokens", "token_count"]
        content_patterns: ["integer", "range 0-1000000"]
        field_names: ["prompt_tokens", "completion_tokens", "input_tokens"]
      
    model_identifier:
      indicators:
        path_patterns: ["model", "model_name", "modelId"]
        content_patterns: ["string", "contains dash or underscore"]
        prefixes: ["gpt-", "claude-", "gemini-", "llama-"]
```

#### **1.3 Content Extraction DSL**
```yaml
# llm_response_discovery/content_extraction.yaml
content_extraction:
  version: "1.0"
  
  # Extraction rules for different content types
  extraction_rules:
    message_extraction:
      simple_text:
        source_path: "content"
        extraction_method: "direct"
        
      array_text:
        source_path: "content.*.text"
        extraction_method: "concatenate"
        separator: " "
        
      parts_array:
        source_path: "parts.*.text"
        extraction_method: "concatenate"
        separator: " "
    
    usage_extraction:
      openai_format:
        prompt_tokens: "usage.prompt_tokens"
        completion_tokens: "usage.completion_tokens"
        total_tokens: "usage.total_tokens"
        
      anthropic_format:
        prompt_tokens: "usage.input_tokens"
        completion_tokens: "usage.output_tokens"
        total_tokens: "calculated"
```

### **Component 2: Semantic Convention Mapping DSL**

#### **2.1 OpenInference Mapping DSL**
```yaml
# semantic_convention_mapping/openinference_mapping.yaml
openinference_mapping:
  version: "1.0"
  target_convention: "openinference"
  
  # Map normalized fields to OpenInference attributes
  field_mappings:
    message_content:
      input_messages:
        target_attribute: "llm.input_messages"
        format: "array_of_objects"
        structure:
          role: "extracted_role"
          content: "extracted_content"
      
      output_messages:
        target_attribute: "llm.output_messages"
        format: "array_of_objects"
        structure:
          role: "assistant"
          content: "extracted_content"
    
    token_usage:
      prompt_tokens:
        target_attribute: "llm.token_count_prompt"
        format: "integer"
        source: "normalized_usage.prompt_tokens"
        
      completion_tokens:
        target_attribute: "llm.token_count_completion"
        format: "integer"
        source: "normalized_usage.completion_tokens"
    
    model_info:
      model_name:
        target_attribute: "llm.model_name"
        format: "string"
        source: "normalized_model.name"
```

#### **2.2 Traceloop Mapping DSL**
```yaml
# semantic_convention_mapping/traceloop_mapping.yaml
traceloop_mapping:
  version: "1.0"
  target_convention: "traceloop"
  
  # Map normalized fields to Traceloop Gen AI attributes
  field_mappings:
    model_info:
      request_model:
        target_attribute: "gen_ai.request.model"
        format: "string"
        source: "normalized_model.name"
        
      response_model:
        target_attribute: "gen_ai.response.model"
        format: "string"
        source: "normalized_model.name"
    
    token_usage:
      prompt_tokens:
        target_attribute: "gen_ai.usage.prompt_tokens"
        format: "integer"
        source: "normalized_usage.prompt_tokens"
        
      completion_tokens:
        target_attribute: "gen_ai.usage.completion_tokens"
        format: "integer"
        source: "normalized_usage.completion_tokens"
    
    message_content:
      system_message:
        target_attribute: "gen_ai.system"
        format: "string"
        source: "normalized_messages.system"
        
      completion:
        target_attribute: "gen_ai.completion"
        format: "string"
        source: "normalized_messages.assistant"
```

#### **2.3 HoneyHive Schema Mapping DSL**
```yaml
# semantic_convention_mapping/honeyhive_mapping.yaml
honeyhive_mapping:
  version: "1.0"
  target_convention: "honeyhive"
  
  # Map normalized fields to HoneyHive four-section schema
  section_mappings:
    inputs:
      chat_history:
        source: "normalized_messages.conversation"
        format: "message_array"
        
      system_prompt:
        source: "normalized_messages.system"
        format: "string"
    
    outputs:
      content:
        source: "normalized_messages.assistant"
        format: "string"
        
      tool_calls:
        source: "normalized_tool_calls"
        format: "tool_call_array"
        
      finish_reason:
        source: "normalized_completion.finish_reason"
        format: "string"
    
    config:
      model:
        source: "normalized_model.name"
        format: "string"
        
      parameters:
        source: "normalized_config"
        format: "object"
    
    metadata:
      usage:
        source: "normalized_usage"
        format: "usage_object"
        
      provider:
        source: "detected_provider.name"
        format: "string"
        
      processing_metrics:
        source: "processing_context"
        format: "metrics_object"
```

## ðŸ”§ **Implementation Architecture**

### **Processing Pipeline**
```python
# Step 1: LLM Response Structure Discovery
raw_response = get_llm_response()
discovery_engine = LLMResponseDiscoveryEngine(discovery_dsl_config)
normalized_data = discovery_engine.process(raw_response)

# Step 2: Semantic Convention Mapping  
mapping_engine = SemanticConventionMapper(mapping_dsl_config)
openinference_output = mapping_engine.map_to_openinference(normalized_data)
traceloop_output = mapping_engine.map_to_traceloop(normalized_data)
honeyhive_output = mapping_engine.map_to_honeyhive(normalized_data)
```

### **Clear Separation Benefits**
1. **Single Responsibility**: Each DSL component has one clear purpose
2. **Independent Evolution**: LLM discovery can evolve separately from convention mapping
3. **Reusable Components**: Convention mapping works with any normalized data
4. **Testable**: Each component can be tested independently
5. **Maintainable**: Changes to provider formats don't affect convention mapping

## ðŸ“‹ **File Structure Correction**

```
src/honeyhive/tracer/semantic_conventions/
â”œâ”€â”€ llm_response_discovery/           # LLM Response Structure Discovery
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ discovery_engine.py           # Main discovery processor
â”‚   â”œâ”€â”€ structure_analyzer.py         # JSON structure analysis
â”‚   â”œâ”€â”€ field_classifier.py           # Field type classification
â”‚   â”œâ”€â”€ content_extractor.py          # Content extraction logic
â”‚   â””â”€â”€ dsl/
â”‚       â”œâ”€â”€ structure_patterns.yaml   # Provider pattern recognition
â”‚       â”œâ”€â”€ field_classification.yaml # Field type classification rules
â”‚       â””â”€â”€ content_extraction.yaml   # Content extraction rules
â”‚
â”œâ”€â”€ semantic_convention_mapping/      # Semantic Convention Mapping
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ mapping_engine.py             # Main mapping processor
â”‚   â”œâ”€â”€ openinference_mapper.py       # OpenInference convention mapper
â”‚   â”œâ”€â”€ traceloop_mapper.py           # Traceloop convention mapper
â”‚   â”œâ”€â”€ honeyhive_mapper.py           # HoneyHive schema mapper
â”‚   â””â”€â”€ dsl/
â”‚       â”œâ”€â”€ openinference_mapping.yaml
â”‚       â”œâ”€â”€ traceloop_mapping.yaml
â”‚       â”œâ”€â”€ honeyhive_mapping.yaml
â”‚       â””â”€â”€ openlit_mapping.yaml
â”‚
â””â”€â”€ universal/                        # Orchestration Layer
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ universal_processor.py        # Coordinates both engines
    â””â”€â”€ models.py                     # Shared data models
```

This corrected architecture provides proper separation of concerns with single responsibility for each DSL component.
