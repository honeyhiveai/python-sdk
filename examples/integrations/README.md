# LLM Provider Integration Examples

This directory contains examples for integrating HoneyHive with various LLM providers using the BYOI (Bring Your Own Instrumentor) architecture.

## üîß **Integration Types**

### **OpenInference Instrumentors**
Lightweight, community-driven instrumentors following OpenTelemetry standards:

- **[`openinference_openai_example.py`](openinference_openai_example.py)** - OpenAI integration
- **[`openinference_anthropic_example.py`](openinference_anthropic_example.py)** - Anthropic integration  
- **[`openinference_google_ai_example.py`](openinference_google_ai_example.py)** - Google AI integration
- **[`openinference_google_adk_example.py`](openinference_google_adk_example.py)** - Google Agent Development Kit
- **[`openinference_bedrock_example.py`](openinference_bedrock_example.py)** - AWS Bedrock integration
- **[`openinference_mcp_example.py`](openinference_mcp_example.py)** - MCP (Model Context Protocol) integration

### **Traceloop Instrumentors**
Enhanced instrumentors with production optimizations and extended metrics:

- **[`traceloop_openai_example.py`](traceloop_openai_example.py)** - OpenAI integration
- **[`traceloop_anthropic_example.py`](traceloop_anthropic_example.py)** - Anthropic integration
- **[`traceloop_bedrock_example.py`](traceloop_bedrock_example.py)** - AWS Bedrock integration (‚úÖ multi-model support)
- **[`traceloop_azure_openai_example.py`](traceloop_azure_openai_example.py)** - Azure OpenAI integration (‚úÖ multi-deployment support)
- **[`traceloop_mcp_example.py`](traceloop_mcp_example.py)** - MCP integration (‚úÖ tool orchestration)
- **[`traceloop_google_ai_example.py`](traceloop_google_ai_example.py)** - Google AI integration (‚ö†Ô∏è upstream issue)
- **[`traceloop_google_ai_example_with_workaround.py`](traceloop_google_ai_example_with_workaround.py)** - Google AI with workaround (‚úÖ functional)

### **Agent Framework Integrations**
Comprehensive examples for popular AI agent frameworks:

- **[`openai_agents_integration.py`](openai_agents_integration.py)** - OpenAI Agents SDK with OpenInference instrumentor (‚úÖ multi-agent, handoffs, guardrails, tools)
- **[`semantic_kernel_integration.py`](semantic_kernel_integration.py)** - Microsoft Semantic Kernel with OpenAI instrumentor (‚úÖ agents, plugins, function calling, streaming)
- **[`strands_integration.py`](strands_integration.py)** - AWS Strands with TracerProvider pattern (‚úÖ Bedrock models, streaming, tools)

## üöÄ **Quick Start**

### For Instrumentor-Based Integrations
1. **Choose Your Instrumentor**: OpenInference (lightweight) or Traceloop (enhanced)
2. **Install Dependencies**: Each example includes specific requirements
3. **Set Environment Variables**: API keys and configuration
4. **Run Example**: `python integrations/[example_name].py`

### For Agent Framework Integrations

#### OpenAI Agents SDK
```bash
pip install openai-agents openinference-instrumentation-openai-agents
export OPENAI_API_KEY=sk-...
export HH_API_KEY=your-honeyhive-key
python integrations/openai_agents_integration.py
```

**Features demonstrated:**
- ‚úÖ Basic agent invocation and tracing
- ‚úÖ Multi-agent orchestration with handoffs
- ‚úÖ Tool/function calling with automatic capture
- ‚úÖ Input/output guardrails
- ‚úÖ Structured outputs with Pydantic
- ‚úÖ Streaming responses
- ‚úÖ Custom context and metadata
- ‚úÖ Complex multi-agent workflows

#### Microsoft Semantic Kernel
```bash
pip install semantic-kernel openinference-instrumentation-openai
export OPENAI_API_KEY=sk-...
export HH_API_KEY=your-honeyhive-key
python integrations/semantic_kernel_integration.py
```

**Features demonstrated:**
- ‚úÖ ChatCompletionAgent with plugins
- ‚úÖ Automatic function calling by AI
- ‚úÖ Structured outputs with Pydantic
- ‚úÖ Multi-turn conversations with history
- ‚úÖ Multiple agents with different models
- ‚úÖ Streaming responses with TTFT
- ‚úÖ Multi-agent workflows
- ‚úÖ Plugin development with @kernel_function

#### AWS Strands
```bash
pip install strands boto3
export AWS_ACCESS_KEY_ID=your-access-key
export AWS_SECRET_ACCESS_KEY=your-secret-key
export BEDROCK_MODEL_ID=anthropic.claude-3-haiku-20240307-v1:0
export HH_API_KEY=your-honeyhive-key
python integrations/strands_integration.py
```

**Features demonstrated:**
- ‚úÖ Bedrock model integration
- ‚úÖ Tool execution with agents
- ‚úÖ Streaming mode support
- ‚úÖ Custom trace attributes
- ‚úÖ Structured outputs
- ‚úÖ Event loop cycle tracing

## üìñ **Documentation**

For detailed integration guides, see:
- **[How-To Guides](../../docs/how-to/integrations/)** - Step-by-step integration instructions
- **[Compatibility Matrix](../../docs/explanation/)** - Full compatibility and version support
- **[BYOI Architecture](../../docs/explanation/architecture/)** - Technical architecture details

## üéØ **Integration Pattern**

All examples follow the standard HoneyHive integration pattern:

```python
from honeyhive import HoneyHiveTracer

# Initialize HoneyHive tracer
tracer = HoneyHiveTracer.init(
    api_key="your-api-key",
    project="my-project",  # Required for OTLP tracing
    source="production"
)

# Install your chosen instrumentor
# Your LLM calls are automatically traced!
```

**Choose the integration that best fits your needs!** üöÄ
