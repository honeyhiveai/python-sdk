"""
Metrics Calculator Module

This module implements comprehensive tracer performance metrics calculations
based on teammate feedback formulas. Includes all six north-star metrics and
advanced analysis capabilities. Follows Agent OS production code standards.
"""

import logging
import statistics
from dataclasses import dataclass
from typing import List, Dict, Any, Optional, Tuple
from ..core.metrics import PerformanceMetrics
from ..providers.base_provider import ProviderResponse

logger = logging.getLogger(__name__)


@dataclass
class NorthStarMetrics:
    """Six north-star metrics for tracer capability assessment.
    
    Based on teammate feedback for quick tracer capability assessment covering
    efficiency, fidelity, and reliability without needing full detail.
    
    :param overhead_latency_percent: How much slower traced vs untraced (%)
    :type overhead_latency_percent: float
    :param dropped_span_rate_percent: How many spans lost before storage (%)
    :type dropped_span_rate_percent: float
    :param export_latency_p95_ms: P95 export latency in milliseconds
    :type export_latency_p95_ms: float
    :param trace_coverage_percent: Share of requests with complete root span (%)
    :type trace_coverage_percent: float
    :param attribute_completeness_percent: Spans with all required fields (%)
    :type attribute_completeness_percent: float
    :param memory_overhead_percent: Extra memory footprint from tracer (%)
    :type memory_overhead_percent: float
    
    Example:
        >>> metrics = NorthStarMetrics(
        ...     overhead_latency_percent=0.1,
        ...     dropped_span_rate_percent=0.0,
        ...     export_latency_p95_ms=75.0,
        ...     trace_coverage_percent=100.0,
        ...     attribute_completeness_percent=95.0,
        ...     memory_overhead_percent=1.5
        ... )
        >>> print(f"Tracer overhead: {metrics.overhead_latency_percent}%")
    """
    overhead_latency_percent: float
    dropped_span_rate_percent: float
    export_latency_p95_ms: float
    trace_coverage_percent: float
    attribute_completeness_percent: float
    memory_overhead_percent: float
    
    def is_complete(self) -> bool:
        """Check if all north-star metrics are available.
        
        :return: True if all metrics have valid values
        :rtype: bool
        """
        return all(
            metric is not None and metric >= 0
            for metric in [
                self.overhead_latency_percent,
                self.dropped_span_rate_percent,
                self.export_latency_p95_ms,
                self.trace_coverage_percent,
                self.attribute_completeness_percent,
                self.memory_overhead_percent,
            ]
        )
    
    def get_summary(self) -> Dict[str, str]:
        """Get human-readable summary of north-star metrics.
        
        :return: Dictionary with formatted metric descriptions
        :rtype: Dict[str, str]
        """
        return {
            "Cost of Tracing": f"{self.overhead_latency_percent:.2f}% latency + {self.memory_overhead_percent:.2f}% memory",
            "Fidelity of Data": f"{self.trace_coverage_percent:.1f}% coverage + {self.attribute_completeness_percent:.1f}% completeness",
            "Reliability of Pipeline": f"{self.dropped_span_rate_percent:.1f}% drops + {self.export_latency_p95_ms:.1f}ms P95 export",
        }


class MetricsCalculator:
    """Comprehensive metrics calculator implementing teammate feedback formulas.
    
    Calculates all performance metrics including the six north-star metrics,
    core efficiency metrics, accuracy & fidelity metrics, and reliability metrics.
    Implements formulas from teammate feedback for professional tracer assessment.
    
    Example:
        >>> calculator = MetricsCalculator()
        >>> responses = [...]  # List of ProviderResponse objects
        >>> metrics = calculator.calculate_comprehensive_metrics(
        ...     responses=responses,
        ...     memory_stats=memory_data,
        ...     network_stats=network_data
        ... )
        >>> north_star = calculator.extract_north_star_metrics(metrics)
    """
    
    def __init__(self) -> None:
        """Initialize metrics calculator."""
        logger.debug("ðŸ“Š MetricsCalculator initialized")
    
    def calculate_overhead_latency(self, traced_latencies: List[float], baseline_latencies: Optional[List[float]] = None) -> Tuple[float, float]:
        """Calculate overhead latency (north-star metric #1).
        
        Formula from teammate feedback:
        Î”lat = mean(lat_traced âˆ’ lat_untraced); % = Î”lat / mean(lat_untraced)
        
        :param traced_latencies: List of traced operation latencies in ms
        :type traced_latencies: List[float]
        :param baseline_latencies: List of untraced baseline latencies in ms
        :type baseline_latencies: Optional[List[float]]
        :return: Tuple of (overhead_ms, overhead_percent)
        :rtype: Tuple[float, float]
        
        Example:
            >>> calculator = MetricsCalculator()
            >>> traced = [1000, 1100, 1050]
            >>> baseline = [950, 1000, 980]
            >>> overhead_ms, overhead_pct = calculator.calculate_overhead_latency(traced, baseline)
        """
        if not traced_latencies:
            return 0.0, 0.0
        
        mean_traced = statistics.mean(traced_latencies)
        
        if baseline_latencies and len(baseline_latencies) > 0:
            # A/B comparison with actual baseline
            mean_baseline = statistics.mean(baseline_latencies)
            overhead_ms = mean_traced - mean_baseline
            overhead_percent = (overhead_ms / mean_baseline) * 100 if mean_baseline > 0 else 0.0
        else:
            # Estimate based on span processing time (realistic approach)
            # Typical tracer overhead is 0.1-2% for well-optimized tracers
            estimated_overhead_ms = 1.0  # ~1ms typical span processing
            overhead_ms = estimated_overhead_ms
            overhead_percent = (overhead_ms / mean_traced) * 100 if mean_traced > 0 else 0.0
        
        logger.debug(f"ðŸ“Š Overhead latency: {overhead_ms:.2f}ms ({overhead_percent:.2f}%)")
        return overhead_ms, overhead_percent
    
    def calculate_dropped_span_rate(self, total_operations: int, successful_operations: int) -> float:
        """Calculate dropped span rate (north-star metric #2).
        
        Formula from teammate feedback:
        % = dropped / (exported + dropped)
        
        :param total_operations: Total number of operations attempted
        :type total_operations: int
        :param successful_operations: Number of successful operations
        :type successful_operations: int
        :return: Dropped span rate as percentage
        :rtype: float
        """
        if total_operations <= 0:
            return 0.0
        
        dropped_operations = total_operations - successful_operations
        dropped_rate = (dropped_operations / total_operations) * 100
        
        logger.debug(f"ðŸ“Š Dropped span rate: {dropped_rate:.2f}% ({dropped_operations}/{total_operations})")
        return dropped_rate
    
    def calculate_export_latency_percentiles(self, export_latencies: List[float]) -> Tuple[float, float]:
        """Calculate export latency percentiles (north-star metric #3).
        
        Formula from teammate feedback:
        Span enqueueâ†’ACK from exporter (p50/p95 ms)
        
        :param export_latencies: List of export latencies in seconds
        :type export_latencies: List[float]
        :return: Tuple of (p50_ms, p95_ms)
        :rtype: Tuple[float, float]
        """
        if not export_latencies:
            return 0.0, 0.0
        
        # Convert to milliseconds
        latencies_ms = [lat * 1000 for lat in export_latencies]
        sorted_latencies = sorted(latencies_ms)
        
        # Calculate percentiles with proper bounds checking
        p50_idx = max(0, min(len(sorted_latencies) - 1, int(0.50 * len(sorted_latencies))))
        p95_idx = max(0, min(len(sorted_latencies) - 1, int(0.95 * len(sorted_latencies))))
        
        p50_ms = sorted_latencies[p50_idx]
        p95_ms = sorted_latencies[p95_idx]
        
        logger.debug(f"ðŸ“Š Export latency: P50={p50_ms:.1f}ms, P95={p95_ms:.1f}ms")
        return p50_ms, p95_ms
    
    def calculate_memory_overhead(self, baseline_memory_mb: float, avg_memory_mb: float) -> Tuple[float, float]:
        """Calculate memory overhead (north-star metric #6).
        
        Formula from OpenTelemetry best practices:
        % = (RSS_traced âˆ’ RSS_untraced) / RSS_untraced
        
        Uses average memory instead of peak to measure sustained overhead,
        following OpenTelemetry benchmarking recommendations for dynamic memory consumption.
        
        :param baseline_memory_mb: True untraced baseline memory usage in MB
        :type baseline_memory_mb: float
        :param avg_memory_mb: Average memory usage during tracing operations in MB
        :type avg_memory_mb: float
        :return: Tuple of (overhead_mb, overhead_percent)
        :rtype: Tuple[float, float]
        """
        if baseline_memory_mb <= 0:
            return 0.0, 0.0
        
        overhead_mb = avg_memory_mb - baseline_memory_mb
        overhead_percent = (overhead_mb / baseline_memory_mb) * 100
        
        logger.debug(f"ðŸ“Š Memory overhead (sustained): {overhead_mb:.2f}MB ({overhead_percent:.2f}%)")
        return overhead_mb, overhead_percent
    
    def calculate_latency_statistics(self, latencies: List[float]) -> Dict[str, float]:
        """Calculate comprehensive latency statistics.
        
        :param latencies: List of latencies in milliseconds
        :type latencies: List[float]
        :return: Dictionary with latency statistics
        :rtype: Dict[str, float]
        """
        if not latencies:
            return {
                'avg_latency': 0.0,
                'min_latency': 0.0,
                'max_latency': 0.0,
                'p95_latency': 0.0,
                'p99_latency': 0.0,
                'std_deviation': 0.0,
            }
        
        sorted_latencies = sorted(latencies)
        
        # Calculate percentiles with bounds checking
        p95_idx = max(0, min(len(sorted_latencies) - 1, int(0.95 * len(sorted_latencies))))
        p99_idx = max(0, min(len(sorted_latencies) - 1, int(0.99 * len(sorted_latencies))))
        
        return {
            'avg_latency': statistics.mean(latencies),
            'min_latency': min(latencies),
            'max_latency': max(latencies),
            'p95_latency': sorted_latencies[p95_idx],
            'p99_latency': sorted_latencies[p99_idx],
            'std_deviation': statistics.stdev(latencies) if len(latencies) > 1 else 0.0,
        }
    
    def calculate_throughput_metrics(self, operation_count: int, total_time_seconds: float) -> Dict[str, float]:
        """Calculate throughput and performance metrics.
        
        :param operation_count: Number of operations performed
        :type operation_count: int
        :param total_time_seconds: Total execution time in seconds
        :type total_time_seconds: float
        :return: Dictionary with throughput metrics
        :rtype: Dict[str, float]
        """
        if total_time_seconds <= 0:
            return {
                'operations_per_second': 0.0,
                'avg_time_per_operation': 0.0,
                'total_operations': operation_count,
                'total_time_seconds': total_time_seconds,
            }
        
        ops_per_second = operation_count / total_time_seconds
        avg_time_per_op = total_time_seconds / max(operation_count, 1)
        
        return {
            'operations_per_second': ops_per_second,
            'avg_time_per_operation': avg_time_per_op,
            'total_operations': operation_count,
            'total_time_seconds': total_time_seconds,
        }
    
    def calculate_comprehensive_metrics(
        self,
        responses: List[ProviderResponse],
        memory_stats: Optional[Dict[str, Any]] = None,
        network_stats: Optional[Dict[str, Any]] = None,
        network_io_stats: Optional[Dict[str, Any]] = None,
        trace_validation_stats: Optional[Dict[str, Any]] = None,
        overhead_stats: Optional[Dict[str, Any]] = None,
    ) -> PerformanceMetrics:
        """Calculate comprehensive performance metrics from provider responses.
        
        :param responses: List of provider responses to analyze
        :type responses: List[ProviderResponse]
        :param memory_stats: Memory profiling statistics
        :type memory_stats: Optional[Dict[str, Any]]
        :param network_stats: Network I/O monitoring statistics (export latency)
        :type network_stats: Optional[Dict[str, Any]]
        :param network_io_stats: Network I/O analysis statistics (LLM vs tracer traffic)
        :type network_io_stats: Optional[Dict[str, Any]]
        :param trace_validation_stats: Trace validation statistics
        :type trace_validation_stats: Optional[Dict[str, Any]]
        :return: Comprehensive performance metrics
        :rtype: PerformanceMetrics
        
        Example:
            >>> calculator = MetricsCalculator()
            >>> responses = [response1, response2, response3]
            >>> metrics = calculator.calculate_comprehensive_metrics(
            ...     responses=responses,
            ...     memory_stats=memory_data,
            ...     network_stats=network_data
            ... )
        """
        if not responses:
            logger.warning("No responses provided for metrics calculation")
            return self._create_empty_metrics()
        
        # Extract basic data
        provider_name = responses[0].provider_name
        latencies = [r.latency_ms for r in responses]
        successful_responses = [r for r in responses if r.success]
        
        # Calculate latency statistics
        latency_stats = self.calculate_latency_statistics(latencies)
        
        # Calculate throughput metrics
        total_time = sum(latencies) / 1000  # Convert to seconds
        throughput_stats = self.calculate_throughput_metrics(len(responses), total_time)
        
        # Calculate overhead latency using real span interceptor data
        if overhead_stats and overhead_stats.get('avg_real_tracer_overhead_ms', 0) > 0:
            # Use real tracer overhead from span interceptor
            overhead_ms = overhead_stats['avg_real_tracer_overhead_ms']
            mean_latency = statistics.mean(latencies)
            overhead_percent = (overhead_ms / mean_latency) * 100 if mean_latency > 0 else 0.0
            logger.debug(f"ðŸ“Š Using real tracer overhead: {overhead_ms:.3f}ms ({overhead_percent:.2f}%)")
        else:
            # Fallback to span processing times from responses
            span_processing_times = [r.span_processing_time_ms for r in responses if r.span_processing_time_ms is not None]
            if span_processing_times:
                overhead_ms = statistics.mean(span_processing_times)
                mean_latency = statistics.mean(latencies)
                overhead_percent = (overhead_ms / mean_latency) * 100 if mean_latency > 0 else 0.0
                logger.debug(f"ðŸ“Š Using simulated span processing time: {overhead_ms:.3f}ms ({overhead_percent:.2f}%)")
            else:
                overhead_ms, overhead_percent = self.calculate_overhead_latency(latencies)
                logger.debug(f"ðŸ“Š Using estimated overhead: {overhead_ms:.3f}ms ({overhead_percent:.2f}%)")
        
        # Calculate success/failure metrics
        success_count = len(successful_responses)
        success_rate = (success_count / len(responses)) * 100
        dropped_rate = self.calculate_dropped_span_rate(len(responses), success_count)
        
        # Memory metrics (using average for sustained overhead measurement)
        memory_baseline = memory_stats.get('baseline_memory_mb', 0.0) if memory_stats else 0.0
        memory_avg = memory_stats.get('avg_memory_mb', 0.0) if memory_stats else 0.0
        memory_peak = memory_stats.get('peak_memory_mb', 0.0) if memory_stats else 0.0
        memory_overhead_mb, memory_overhead_percent = self.calculate_memory_overhead(memory_baseline, memory_avg)
        
        # Network metrics
        export_latencies = network_stats.get('export_latency_samples', []) if network_stats else []
        _, export_p95 = self.calculate_export_latency_percentiles(export_latencies)
        
        # Detailed Network I/O Analysis
        llm_request_avg_kb = network_io_stats.get('llm_request_avg_kb', 0.0) if network_io_stats else 0.0
        llm_response_avg_kb = network_io_stats.get('llm_response_avg_kb', 0.0) if network_io_stats else 0.0
        llm_latency_avg_ms = network_io_stats.get('llm_latency_avg_ms', 0.0) if network_io_stats else 0.0
        tracer_export_avg_kb = network_io_stats.get('tracer_export_avg_kb', 0.0) if network_io_stats else 0.0
        tracer_export_latency_avg_ms = network_io_stats.get('tracer_export_latency_avg_ms', 0.0) if network_io_stats else 0.0
        total_llm_traffic_kb = network_io_stats.get('total_llm_traffic_kb', 0.0) if network_io_stats else 0.0
        total_tracer_traffic_kb = network_io_stats.get('total_tracer_traffic_kb', 0.0) if network_io_stats else 0.0
        tracer_traffic_percent = network_io_stats.get('tracer_traffic_percent', 0.0) if network_io_stats else 0.0
        
        # Trace validation metrics
        trace_coverage = trace_validation_stats.get('trace_coverage_percent', 0.0) if trace_validation_stats else 0.0
        attribute_completeness = trace_validation_stats.get('attribute_completeness_percent', 0.0) if trace_validation_stats else 0.0
        
        # Create comprehensive metrics
        metrics = PerformanceMetrics(
            provider=provider_name,
            mode="calculated",  # Will be updated by caller
            total_operations=len(responses),
            total_time=total_time,
            operations_per_second=throughput_stats['operations_per_second'],
            avg_latency=latency_stats['avg_latency'],
            min_latency=latency_stats['min_latency'],
            max_latency=latency_stats['max_latency'],
            p95_latency=latency_stats['p95_latency'],
            p99_latency=latency_stats['p99_latency'],
            tracer_overhead=overhead_ms,
            tracer_overhead_percent=overhead_percent,
            span_processing_time_ms=overhead_ms if overhead_stats else 1.0,
            network_requests_count=network_stats.get('total_requests', 0) if network_stats else 0,
            network_bytes_sent=network_stats.get('bytes_sent', 0) if network_stats else 0,
            network_bytes_received=network_stats.get('bytes_received', 0) if network_stats else 0,
            network_export_latency_ms=export_p95,
            network_bytes_per_operation=network_stats.get('bytes_per_operation', 0.0) if network_stats else 0.0,
            llm_request_avg_kb=llm_request_avg_kb,
            llm_response_avg_kb=llm_response_avg_kb,
            llm_latency_avg_ms=llm_latency_avg_ms,
            tracer_export_avg_kb=tracer_export_avg_kb,
            tracer_export_latency_avg_ms=tracer_export_latency_avg_ms,
            total_llm_traffic_kb=total_llm_traffic_kb,
            total_tracer_traffic_kb=total_tracer_traffic_kb,
            tracer_traffic_percent=tracer_traffic_percent,
            memory_baseline_mb=memory_baseline,
            memory_peak_mb=memory_peak,
            memory_overhead_mb=memory_overhead_mb,
            memory_overhead_percent=memory_overhead_percent,
            success_rate=success_rate,
            error_count=len(responses) - success_count,
            trace_coverage_percent=trace_coverage,
            attribute_completeness_percent=attribute_completeness,
            dropped_span_rate_percent=dropped_rate,
        )
        
        logger.debug(f"ðŸ“Š Comprehensive metrics calculated for {provider_name}: {len(responses)} operations")
        return metrics
    
    def extract_north_star_metrics(self, metrics: PerformanceMetrics) -> NorthStarMetrics:
        """Extract the six north-star metrics from comprehensive metrics.
        
        :param metrics: Comprehensive performance metrics
        :type metrics: PerformanceMetrics
        :return: North-star metrics for quick assessment
        :rtype: NorthStarMetrics
        """
        return NorthStarMetrics(
            overhead_latency_percent=metrics.tracer_overhead_percent,
            dropped_span_rate_percent=metrics.dropped_span_rate_percent or (100.0 - metrics.success_rate),
            export_latency_p95_ms=metrics.network_export_latency_ms,
            trace_coverage_percent=metrics.trace_coverage_percent or 0.0,
            attribute_completeness_percent=metrics.attribute_completeness_percent or 0.0,
            memory_overhead_percent=metrics.memory_overhead_percent,
        )
    
    def _create_empty_metrics(self) -> PerformanceMetrics:
        """Create empty metrics for error cases.
        
        :return: Empty performance metrics
        :rtype: PerformanceMetrics
        """
        return PerformanceMetrics(
            provider="unknown",
            mode="empty",
            total_operations=0,
            total_time=0.0,
            operations_per_second=0.0,
            avg_latency=0.0,
            min_latency=0.0,
            max_latency=0.0,
            p95_latency=0.0,
            p99_latency=0.0,
            tracer_overhead=0.0,
            tracer_overhead_percent=0.0,
            span_processing_time_ms=0.0,
            network_requests_count=0,
            network_bytes_sent=0,
            network_bytes_received=0,
            network_export_latency_ms=0.0,
            network_bytes_per_operation=0.0,
            memory_baseline_mb=0.0,
            memory_peak_mb=0.0,
            memory_overhead_mb=0.0,
            memory_overhead_percent=0.0,
            success_rate=0.0,
            error_count=0,
        )
    
    def compare_providers(self, metrics_list: List[PerformanceMetrics]) -> Dict[str, Any]:
        """Compare performance metrics across multiple providers.
        
        :param metrics_list: List of performance metrics to compare
        :type metrics_list: List[PerformanceMetrics]
        :return: Comparison analysis
        :rtype: Dict[str, Any]
        """
        if len(metrics_list) < 2:
            logger.warning("Need at least 2 metrics for comparison")
            return {}
        
        comparison = {
            'providers': [m.provider for m in metrics_list],
            'latency_comparison': {},
            'throughput_comparison': {},
            'overhead_comparison': {},
            'reliability_comparison': {},
        }
        
        # Find best/worst performers
        best_latency = min(metrics_list, key=lambda m: m.avg_latency)
        best_throughput = max(metrics_list, key=lambda m: m.operations_per_second)
        lowest_overhead = min(metrics_list, key=lambda m: m.tracer_overhead_percent)
        highest_reliability = max(metrics_list, key=lambda m: m.success_rate)
        
        comparison.update({
            'best_latency_provider': best_latency.provider,
            'best_throughput_provider': best_throughput.provider,
            'lowest_overhead_provider': lowest_overhead.provider,
            'highest_reliability_provider': highest_reliability.provider,
        })
        
        logger.debug(f"ðŸ“Š Provider comparison completed for {len(metrics_list)} providers")
        return comparison
