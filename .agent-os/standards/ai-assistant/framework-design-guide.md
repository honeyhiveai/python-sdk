# Agent OS Framework Design Guide

**How to Build Procedural APIs for AI Assistants**

**Version**: 1.0  
**Date**: 2025-09-30  
**Purpose**: Systematic guide for designing deterministic, high-quality AI workflows using Agent OS patterns

---

## ğŸ¯ **Core Concept: Frameworks as Procedural APIs**

Agent OS frameworks are **procedural APIs** for AI assistants, not documentation. This is a paradigm shift from traditional prompt engineering to systematic workflow design.

### **The API Paradigm**

| Traditional Programming API | Agent OS Workflow API |
|-----------------------------|----------------------|
| **Function Definition** | Task file with command sequences |
| **Function Call** | `ğŸ¯ NEXT-MANDATORY: [file.md]` navigation |
| **Return Value** | `ğŸ“Š COUNT-AND-DOCUMENT: result` evidence |
| **Conditional Logic** | `ğŸ›‘ VALIDATE-GATE: criteria` validation |
| **Error Handling** | `ğŸš¨ FRAMEWORK-VIOLATION: condition` detection |
| **Loop/Iteration** | Horizontal file scaling within phase |
| **State Management** | Progress table with `ğŸ›‘ UPDATE-TABLE` |
| **Type System** | Command language definitions from glossary |

### **Key Insight**

Traditional approach (fails):
```markdown
"Please analyze the codebase and document your findings."
```

API approach (succeeds):
```markdown
ğŸ›‘ EXECUTE-NOW: grep -n "^class" src/module.py
ğŸ“Š COUNT-AND-DOCUMENT: Classes found: [NUMBER]
ğŸ›‘ VALIDATE-GATE: All classes catalogued âœ…/âŒ
ğŸ¯ NEXT-MANDATORY: [next-task.md]
```

**Why it works**: Binding commands + explicit navigation + evidence requirements = deterministic execution

---

## ğŸ“‹ **Framework Design Process**

### **Step 1: Define the Workflow (API Contract)**

**Map your process to a procedural API with clear phases:**

#### **Example: Provider DSL Development**

```python
# Conceptual API contract
def provider_dsl_workflow(provider_name: str) -> ProviderDSL:
    """Complete provider DSL development workflow"""
    
    # Phase 0: Setup
    research_sources = setup_research_environment(provider_name)
    
    # Phase 1: Official Documentation Discovery
    docs = discover_official_documentation(provider_name)
    
    # Phase 2: Instrumentor Verification
    instrumentors = verify_instrumentor_support(provider_name)
    
    # Phase 3: Model & Pricing Collection
    pricing = collect_model_pricing(provider_name, docs)
    
    # Phase 4: Structure Patterns
    patterns = create_detection_patterns(provider_name, instrumentors)
    
    # Phase 5: Navigation Rules
    rules = create_navigation_rules(provider_name, instrumentors)
    
    # Phase 6: Field Mappings
    mappings = create_field_mappings(provider_name, rules)
    
    # Phase 7: Transforms
    transforms = create_transform_functions(provider_name, pricing)
    
    # Phase 8: Compilation & Validation
    bundle = compile_and_validate(patterns, rules, mappings, transforms)
    
    # Phase 9: Documentation
    finalize_documentation(research_sources, bundle)
    
    return bundle
```

#### **Translation to Agent OS Structure**

```
provider-dsl-development/
â”œâ”€â”€ README.md                    # API documentation (Tier 2)
â”œâ”€â”€ entry-point.md              # main() function entry
â”œâ”€â”€ progress-tracking.md        # State management
â””â”€â”€ phases/
    â”œâ”€â”€ 0/  # setup_research_environment()
    â”œâ”€â”€ 1/  # discover_official_documentation()
    â”œâ”€â”€ 2/  # verify_instrumentor_support()
    â”œâ”€â”€ 3/  # collect_model_pricing()
    â”œâ”€â”€ 4/  # create_detection_patterns()
    â”œâ”€â”€ 5/  # create_navigation_rules()
    â”œâ”€â”€ 6/  # create_field_mappings()
    â”œâ”€â”€ 7/  # create_transform_functions()
    â”œâ”€â”€ 8/  # compile_and_validate()
    â””â”€â”€ 9/  # finalize_documentation()
```

### **Step 2: Design Phase Structure (Modules)**

**Each phase directory is a module containing function definitions:**

#### **Phase as Module Design**

```
phases/2/  # Instrumentor Verification Module
â”œâ”€â”€ shared-analysis.md              # Module entry point / router (â‰¤100 lines)
â”œâ”€â”€ traceloop-verification.md       # verify_traceloop() function
â”œâ”€â”€ openinference-verification.md   # verify_openinference() function
â”œâ”€â”€ openlit-verification.md         # verify_openlit() function
â””â”€â”€ evidence-collection.md          # collect_evidence() - returns to caller
```

#### **Module Responsibilities**

**shared-analysis.md** (Router/Orchestrator):
- Entry point for the phase
- Lists all tasks (functions) to execute
- Routes to first task
- Defines phase completion gate
- Size: â‰¤100 lines

**Task Files** (Functions):
- Single responsibility (one specific action)
- Explicit commands (ğŸ›‘ EXECUTE-NOW)
- Evidence documentation (ğŸ“Š COUNT-AND-DOCUMENT)
- Validation gate (ğŸ›‘ VALIDATE-GATE)
- Navigation to next (ğŸ¯ NEXT-MANDATORY)
- Size: â‰¤100 lines (â‰¤150 for complex tasks)

**Last Task File** (Phase Return):
- Completes final task
- Phase completion gate
- Evidence summary
- Routes to next phase
- Size: â‰¤100 lines

### **Step 3: Write Task Files (Function Definitions)**

#### **Standard Task File Template**

```markdown
# Task X.Y: [Function Name]

**ğŸ¯ [Brief description of what this function does]**

## ğŸš¨ **ENTRY REQUIREMENTS** (preconditions)

ğŸ›‘ VALIDATE-GATE: Task Prerequisites
- [ ] Required input X available âœ…/âŒ
- [ ] State Y confirmed âœ…/âŒ
- [ ] Phase X.Y-1 complete âœ…/âŒ

## ğŸ›‘ **EXECUTION** (function body)

### **Step 1: [Action Name]**
ğŸ›‘ EXECUTE-NOW: [Specific command or action]
```bash
# Example command
curl -s https://api.example.com/endpoint | jq '.field'
```
ğŸ›‘ PASTE-OUTPUT: Complete command results

### **Step 2: [Analysis Name]**
ğŸ“Š COUNT-AND-DOCUMENT: [Specific metric]: [EXACT VALUE]
ğŸ“Š QUANTIFY-RESULTS: [Measurement]: [NUMBER or YES/NO]

### **Step 3: [Documentation Name]**
âš ï¸ EVIDENCE-REQUIRED: [Specific evidence type]
```markdown
# Document findings in specific format
- Field 1: [value]
- Field 2: [value]
```

## ğŸ›‘ **VALIDATION GATE** (postconditions)

ğŸ›‘ VALIDATE-GATE: Task X.Y Complete
- [ ] Command executed with full output âœ…/âŒ
- [ ] Results quantified and documented âœ…/âŒ
- [ ] Evidence meets requirements âœ…/âŒ
- [ ] Quality criteria satisfied âœ…/âŒ

ğŸš¨ FRAMEWORK-VIOLATION: If proceeding without complete evidence

## ğŸ¯ **NAVIGATION** (control flow)

ğŸ›‘ UPDATE-TABLE: Phase X.Y â†’ Complete with evidence
ğŸ¯ NEXT-MANDATORY: [next-task-file.md](next-task-file.md)
```

#### **Last Task File Template (Phase Completion)**

```markdown
# Task X.Z: [Final Task Name]

**ğŸ¯ [Description - this completes Phase X]**

## ğŸš¨ **ENTRY REQUIREMENTS**
[... standard entry requirements ...]

## ğŸ›‘ **EXECUTION**
[... standard execution steps ...]

## ğŸ›‘ **VALIDATION GATE**
[... standard validation gate ...]

## ğŸ›¤ï¸ **PHASE X COMPLETION GATE**

ğŸ›‘ UPDATE-TABLE: Phase X â†’ COMPLETE with comprehensive evidence

### **Phase X Summary**
ğŸ“Š QUANTIFY-RESULTS: Total tasks completed: [X/X]
ğŸ“Š QUANTIFY-RESULTS: Evidence items documented: [NUMBER]
âš ï¸ EVIDENCE-REQUIRED: All validation gates passed

### **Handoff to Phase Y Validated**
âœ… **Output 1**: [Description] - available for Phase Y
âœ… **Output 2**: [Description] - available for Phase Y
âœ… **Output 3**: [Description] - available for Phase Y

### **Phase Y Inputs Available and Verified**
âœ… Input requirement 1 satisfied
âœ… Input requirement 2 satisfied
âœ… Input requirement 3 satisfied

## ğŸ¯ **CROSS-PHASE NAVIGATION**

ğŸ¯ NEXT-MANDATORY: Phase Y [Description] (only after all validation gates pass)
ğŸš¨ FRAMEWORK-VIOLATION: If advancing to Phase Y without Phase X completion
```

### **Step 4: Implement Navigation (Control Flow)**

#### **Within-Phase Navigation (Sequential)**

Standard sequential task chain:

```markdown
# shared-analysis.md routes to first task
ğŸ¯ NEXT-MANDATORY: [task-1.md](task-1.md)

# task-1.md routes to task-2
ğŸ¯ NEXT-MANDATORY: [task-2.md](task-2.md)

# task-2.md routes to task-3
ğŸ¯ NEXT-MANDATORY: [task-3.md](task-3.md)

# task-3.md (last task) routes to next phase
ğŸ¯ NEXT-MANDATORY: Phase Y Description
```

**Navigation Chain Example:**

```
shared-analysis.md
    â†“
    ğŸ¯ NEXT-MANDATORY
    â†“
task-1-setup.md
    â†“
    ğŸ¯ NEXT-MANDATORY
    â†“
task-2-analysis.md
    â†“
    ğŸ¯ NEXT-MANDATORY
    â†“
task-3-validation.md
    â†“
    ğŸ¯ NEXT-MANDATORY: Phase Y
```

#### **Cross-Phase Navigation**

**Only the LAST task file** in a phase routes to the next phase:

```markdown
# phases/2/evidence-collection.md (last task in Phase 2)

ğŸ¯ NEXT-MANDATORY: Phase 3 Model & Pricing Collection (only after all validation gates pass)
```

**NOT in shared-analysis.md or other files** - maintains clear separation of concerns.

#### **Conditional Navigation (Branching)**

For path selection (e.g., unit vs integration tests):

```markdown
# shared-analysis.md

## ğŸ›¤ï¸ **PATH SELECTION**

**Choose ONE path based on test type:**

### **Path A: Unit Tests**
- Strategy: Mock everything external
- Coverage target: 90%+ line/branch
- ğŸ¯ NEXT-MANDATORY: [unit-mock-strategy.md](unit-mock-strategy.md)

### **Path B: Integration Tests**
- Strategy: Real API usage
- Coverage target: Functional flow coverage
- ğŸ¯ NEXT-MANDATORY: [integration-real-strategy.md](integration-real-strategy.md)

âš ï¸ MUST-COMPLETE: Selected path before Phase Y
```

Both paths eventually converge to phase completion:

```
shared-analysis.md
    â”œâ”€â”€ Path A â†’ unit-mock-strategy.md â”€â”€â”
    â”‚                                     â”œâ†’ evidence-collection.md â†’ Phase Y
    â””â”€â”€ Path B â†’ integration-real-strategy.md â”˜
```

### **Step 5: Define Evidence Format (Return Types)**

#### **Standard Evidence Patterns**

**Numerical Evidence:**
```markdown
ğŸ“Š COUNT-AND-DOCUMENT: Classes found: 15
ğŸ“Š COUNT-AND-DOCUMENT: Functions analyzed: 42
ğŸ“Š COUNT-AND-DOCUMENT: Dependencies mapped: 23
```

**Boolean Evidence:**
```markdown
ğŸ“Š QUANTIFY-RESULTS: Traceloop support verified: YES
ğŸ“Š QUANTIFY-RESULTS: Compilation successful: YES
ğŸ“Š QUANTIFY-RESULTS: All tests passing: NO (3 failures)
```

**List Evidence:**
```markdown
âš ï¸ EVIDENCE-REQUIRED: URLs verified:
- https://docs.provider.com/api
- https://docs.provider.com/models
- https://docs.provider.com/pricing
```

**Structured Evidence:**
```markdown
ğŸ“Š QUANTIFY-RESULTS: Instrumentor support matrix:
- Traceloop: âœ… VERIFIED (package exists: opentelemetry-instrumentation-provider)
- OpenInference: âœ… VERIFIED (generic LLM support confirmed)
- OpenLit: âŒ NOT SUPPORTED (no provider directory found)
```

**Command Output Evidence:**
```markdown
ğŸ›‘ PASTE-OUTPUT: AST analysis results
```
CLASS: MyClass (Line 15) - 8 methods
  __init__(self, config) - Line 16 - PUBLIC - Required: 1
  process(self, data) - Line 24 - PUBLIC - Required: 1
  _validate(self, item) - Line 42 - PRIVATE - Required: 1
```
```

#### **Evidence Quality Requirements**

- âœ… **Quantified**: Numbers, not "many" or "several"
- âœ… **Specific**: Exact values, not ranges
- âœ… **Verifiable**: Can be validated by re-running commands
- âœ… **Complete**: All requested evidence provided
- âŒ **Vague**: "Analysis complete", "looks good"
- âŒ **Qualitative**: "high quality", "comprehensive"

### **Step 6: Create Progress Table (State Management)**

#### **Progress Table Template**

```markdown
# Progress Tracking Template

## ğŸ›‘ **MANDATORY: Copy This Table to Chat Before Execution**

| Phase | Status | Evidence | Commands | Gate |
|-------|--------|----------|----------|------|
| 0. Setup | â³ PENDING | 0/3 items | 0/3 | âŒ |
| 1. Discovery | â³ PENDING | 0/4 URLs | 0/4 | âŒ |
| 2. Verification | â³ PENDING | 0/3 verified | 0/8 | âŒ |
| 3. Collection | â³ PENDING | 0 models | 0/6 | âŒ |
| 4. Patterns | â³ PENDING | 0 patterns | 0/5 | âŒ |
| 5. Rules | â³ PENDING | 0 rules | 0/7 | âŒ |
| 6. Mappings | â³ PENDING | 0/4 sections | 0/4 | âŒ |
| 7. Transforms | â³ PENDING | 0 transforms | 0/6 | âŒ |
| 8. Validation | â³ PENDING | 0% pass | 0/5 | âŒ |
| 9. Documentation | â³ PENDING | Incomplete | 0/4 | âŒ |

**Status Legend:**
- â³ PENDING - Not started
- ğŸ”„ IN PROGRESS - Currently executing
- âœ… COMPLETE - All validation gates passed
- âŒ BLOCKED - Waiting for dependency

**Gate Legend:**
- âœ… PASSED - All validation criteria met
- âŒ FAILED - Validation criteria not met
- â³ PENDING - Not yet validated
```

#### **Table Update Pattern**

After each task:
```markdown
ğŸ›‘ UPDATE-TABLE: Phase 2.1 â†’ Traceloop verification complete
```

Update to:
```markdown
| 2. Verification | ğŸ”„ IN PROGRESS | 1/3 verified | 3/8 | â³ |
```

After phase completion:
```markdown
ğŸ›‘ UPDATE-TABLE: Phase 2 â†’ COMPLETE with comprehensive evidence
```

Update to:
```markdown
| 2. Verification | âœ… COMPLETE | 3/3 verified | 8/8 | âœ… |
```

---

## ğŸ¨ **Common Design Patterns**

### **Pattern 1: Sequential Pipeline**

**Structure:**
```
Task 1 â†’ Task 2 â†’ Task 3 â†’ Task 4
```

**Use When:**
- Linear workflow with dependencies
- Each task builds on previous results
- No branching or parallelism needed

**Example:**
```
phases/1/
â”œâ”€â”€ shared-analysis.md â†’ task-1-api-docs.md
â””â”€â”€ task-1-api-docs.md â†’ task-2-models.md
    â””â”€â”€ task-2-models.md â†’ task-3-pricing.md
        â””â”€â”€ task-3-pricing.md â†’ task-4-changelog.md
            â””â”€â”€ task-4-changelog.md â†’ Phase 2
```

### **Pattern 2: Parallel Tasks with Merge**

**Structure:**
```
        â”Œâ†’ Task 2A â”
Task 1 â”€â”¼â†’ Task 2B â”¼â†’ Task 3 (merge evidence)
        â””â†’ Task 2C â”˜
```

**Use When:**
- Independent tasks can execute in any order
- Results need to be aggregated
- No dependencies between parallel tasks

**Implementation:**
```markdown
# shared-analysis.md

## ğŸ›‘ **PARALLEL TASK EXECUTION**

Execute these tasks in any order:

### **Task 2.1: Traceloop Verification**
âš ï¸ MUST-COMPLETE: [traceloop-verification.md](traceloop-verification.md)

### **Task 2.2: OpenInference Verification**
âš ï¸ MUST-COMPLETE: [openinference-verification.md](openinference-verification.md)

### **Task 2.3: OpenLit Verification**
âš ï¸ MUST-COMPLETE: [openlit-verification.md](openlit-verification.md)

ğŸ¯ NEXT-MANDATORY: [evidence-collection.md](evidence-collection.md) (after ALL tasks complete)
```

Each parallel task ends with:
```markdown
ğŸ¯ NEXT-MANDATORY: Return to [shared-analysis.md](shared-analysis.md) for next task
```

### **Pattern 3: Conditional Branch**

**Structure:**
```
        â”Œâ†’ Path A â†’ Task 2A â”
Task 1 â”€â”¤                    â”œâ†’ Task 3
        â””â†’ Path B â†’ Task 2B â”˜
```

**Use When:**
- Different strategies based on input
- Path selection affects execution
- Both paths converge to same endpoint

**Example: Unit vs Integration Tests**
```markdown
# shared-analysis.md

## ğŸ›¤ï¸ **PATH SELECTION**

### **Path A: Unit Tests**
ğŸ¯ NEXT-MANDATORY: [unit-mock-strategy.md](unit-mock-strategy.md)

### **Path B: Integration Tests**
ğŸ¯ NEXT-MANDATORY: [integration-real-strategy.md](integration-real-strategy.md)
```

Both paths end with:
```markdown
ğŸ¯ NEXT-MANDATORY: [evidence-collection.md](evidence-collection.md)
```

### **Pattern 4: Validation Loop with Retry**

**Structure:**
```
Task 1 â†’ Validate â†’ [Pass] â†’ Task 2
              â†“
           [Fail] â†’ Fix Instructions â†’ Task 1
```

**Use When:**
- Quality gates with potential failures
- Iterative improvement needed
- Can't proceed without validation

**Implementation:**
```markdown
# task-validation.md

## ğŸ›‘ **VALIDATION GATE**

ğŸ›‘ VALIDATE-GATE: Quality Criteria
- [ ] Criterion 1 met âœ…/âŒ
- [ ] Criterion 2 met âœ…/âŒ

### **If ALL âœ…:**
ğŸ¯ NEXT-MANDATORY: [next-task.md](next-task.md)

### **If ANY âŒ:**
ğŸš¨ FRAMEWORK-VIOLATION: Quality criteria not met

**Fix Instructions:**
1. Review failed criterion
2. Re-execute [task-X.md](task-X.md)
3. Return to this validation

ğŸ¯ NEXT-MANDATORY: [task-X.md](task-X.md) (for rework)
```

### **Pattern 5: Horizontal Scaling (Extensible Module)**

**Structure:**
```
phases/2/
â”œâ”€â”€ shared-analysis.md
â”œâ”€â”€ task-1.md
â”œâ”€â”€ task-2.md
â”œâ”€â”€ task-3.md
â””â”€â”€ [easy to add task-4.md, task-5.md, etc.]
```

**Use When:**
- Framework may grow over time
- Don't know all requirements upfront
- Want to avoid refactoring when adding features

**Benefits:**
- Add new file â†’ update navigation â†’ done!
- No file size bloat
- Each file stays â‰¤100 lines
- Easy maintenance

**Example: Adding New Instrumentor**
```bash
# Original:
phases/2/
â”œâ”€â”€ traceloop-verification.md
â”œâ”€â”€ openinference-verification.md
â””â”€â”€ openlit-verification.md

# Add new instrumentor - just create new file:
phases/2/
â”œâ”€â”€ traceloop-verification.md
â”œâ”€â”€ openinference-verification.md
â”œâ”€â”€ openlit-verification.md
â””â”€â”€ llamaindex-verification.md  # NEW!
```

Update navigation:
```markdown
# openlit-verification.md
ğŸ¯ NEXT-MANDATORY: [llamaindex-verification.md](llamaindex-verification.md)

# llamaindex-verification.md
ğŸ¯ NEXT-MANDATORY: [evidence-collection.md](evidence-collection.md)
```

---

## ğŸ“ **File Size Guidelines (Context Optimization)**

### **Size Constraints by Tier**

| File Type | Tier | Target Size | Practical Limit | Purpose |
|-----------|------|-------------|-----------------|---------|
| **Hub README** | 2 | 300-400 lines | 500 lines | Overview & discovery |
| **Entry Point** | 1 | 50-75 lines | 100 lines | Framework initialization |
| **Phase Reference** | 2 | 200-300 lines | 400 lines | Optional detailed guide |
| **shared-analysis.md** | 1 | 50-75 lines | 100 lines | Phase router |
| **Task Files** | 1 | 60-80 lines | 100 lines | Specific execution |
| **Complex Tasks** | 1 | 100-120 lines | 150 lines | Multi-step tasks |
| **Progress Table** | 1 | 50-75 lines | 100 lines | State tracking |

### **Why These Limits?**

**From LLM Workflow Engineering Methodology:**

| Context Utilization | Attention Quality | Execution Consistency | File Size |
|---------------------|------------------|----------------------|-----------|
| **15-25%** (Optimal) | 95%+ | 85%+ | â‰¤100 lines |
| **50-75%** (Degraded) | 70-85% | 60-75% | 200-500 lines |
| **75%+** (Failure) | <70% | <50% | 500+ lines |

**Rule of Thumb:**
- Tier 1 (execution): â‰¤100 lines â†’ 15-25% context â†’ 85%+ consistency
- Tier 2 (reference): 200-500 lines â†’ 40-60% context â†’ one-time consumption
- Tier 3 (output): Unlimited â†’ never re-consumed by AI

### **When to Split Files**

**Split if:**
- âœ… File exceeds 100 lines for Tier 1
- âœ… Single task has 3+ distinct sub-tasks
- âœ… File has multiple responsibilities
- âœ… Navigation becomes unclear

**Don't split if:**
- âŒ Creates files <30 lines (too fragmented)
- âŒ Breaks logical cohesion
- âŒ Makes navigation overly complex

---

## âœ… **Quality Checklist for Framework Design**

### **Completeness**

- [ ] Every task file has `ğŸ¯ NEXT-MANDATORY` navigation
- [ ] Every phase has completion gate in last task
- [ ] Every command produces documented evidence
- [ ] Progress table covers all phases
- [ ] All phases have entry requirements
- [ ] All phases have validation gates
- [ ] Hub README provides complete overview
- [ ] Entry point routes to Phase 0

### **Determinism**

- [ ] Commands use binding language (ğŸ›‘, ğŸ“Š, ğŸ¯, âš ï¸)
- [ ] Navigation is explicit with file paths
- [ ] Validation gates have measurable criteria
- [ ] No ambiguous natural language instructions
- [ ] Evidence requirements are quantified
- [ ] All conditionals are explicit
- [ ] Command glossary is referenced
- [ ] Framework violations are detected

### **Maintainability**

- [ ] Horizontal scaling enabled (directories per phase)
- [ ] Each Tier 1 file â‰¤100-150 lines
- [ ] Consistent patterns across all phases
- [ ] Reusable templates applied
- [ ] Clear separation of concerns
- [ ] Modular design allows updates
- [ ] Documentation is co-located
- [ ] Navigation chains are unbroken

### **Discoverability**

- [ ] Entry point clearly identified
- [ ] Hub README provides workflow overview
- [ ] Progress table shows full structure
- [ ] Navigation chain is complete
- [ ] Phase purposes are documented
- [ ] Task responsibilities are clear
- [ ] Command language glossary linked
- [ ] Examples provided where helpful

### **Context Optimization**

- [ ] Tier 1 files in optimal range (â‰¤100 lines)
- [ ] Tier 2 files for reference only (200-500 lines)
- [ ] Command language reduces token usage
- [ ] Evidence format is compact
- [ ] Minimal redundancy across files
- [ ] Clear boundaries between tiers
- [ ] No unnecessary verbose language
- [ ] Focus on execution efficiency

---

## ğŸš€ **Quick Start: Building Your First Framework**

### **Example: API Documentation Generation Framework**

**Goal:** Systematic generation of API documentation from codebase

#### **Step 1: Define API Contract**

```python
def api_documentation_workflow(source_dir: str) -> Documentation:
    # Phase 0: Load codebase context
    context = load_codebase_context(source_dir)
    
    # Phase 1: Analyze codebase structure
    structure = analyze_codebase_structure(context)
    
    # Phase 2: Extract public APIs
    apis = extract_public_apis(structure)
    
    # Phase 3: Generate usage examples
    examples = generate_usage_examples(apis)
    
    # Phase 4: Build documentation
    docs = build_documentation(apis, examples)
    
    # Phase 5: Validate links and references
    validate_documentation(docs)
    
    return docs
```

#### **Step 2: Create Directory Structure**

```bash
api-documentation-framework/
â”œâ”€â”€ README.md
â”œâ”€â”€ entry-point.md
â”œâ”€â”€ progress-tracking.md
â””â”€â”€ phases/
    â”œâ”€â”€ 0/
    â”‚   â”œâ”€â”€ shared-analysis.md
    â”‚   â””â”€â”€ load-context.md
    â”œâ”€â”€ 1/
    â”‚   â”œâ”€â”€ shared-analysis.md
    â”‚   â”œâ”€â”€ analyze-structure.md
    â”‚   â””â”€â”€ inventory-modules.md
    â”œâ”€â”€ 2/
    â”‚   â”œâ”€â”€ shared-analysis.md
    â”‚   â”œâ”€â”€ extract-classes.md
    â”‚   â”œâ”€â”€ extract-functions.md
    â”‚   â””â”€â”€ extract-parameters.md
    â”œâ”€â”€ 3/
    â”‚   â”œâ”€â”€ shared-analysis.md
    â”‚   â”œâ”€â”€ generate-class-examples.md
    â”‚   â””â”€â”€ generate-function-examples.md
    â”œâ”€â”€ 4/
    â”‚   â”œâ”€â”€ shared-analysis.md
    â”‚   â”œâ”€â”€ build-reference.md
    â”‚   â””â”€â”€ build-guides.md
    â””â”€â”€ 5/
        â”œâ”€â”€ shared-analysis.md
        â”œâ”€â”€ validate-links.md
        â””â”€â”€ validate-examples.md
```

#### **Step 3: Write Entry Point**

```markdown
# API Documentation Framework - Entry Point

âš ï¸ MUST-READ: [../command-language-glossary.md](../command-language-glossary.md)

## ğŸ›‘ **MANDATORY STEPS**

### **Step 1: Acknowledge Framework**
ğŸ›‘ EXECUTE-NOW: State acknowledgment
```
âœ… I acknowledge the API Documentation Framework contract:
- I will analyze the codebase systematically
- I will extract ALL public APIs with complete signatures
- I will generate working examples for each API
- I will validate all documentation links
```

### **Step 2: Initialize Progress Table**
ğŸ›‘ UPDATE-TABLE: Copy table to chat
âš ï¸ MUST-READ: [progress-tracking.md](progress-tracking.md)
ğŸ›‘ PASTE-OUTPUT: Complete table in chat

### **Step 3: Execute Framework Phases**
ğŸ¯ NEXT-MANDATORY: [phases/0/shared-analysis.md](phases/0/shared-analysis.md)
```

#### **Step 4: Write First Phase Task**

```markdown
# Phase 0: Load Codebase Context

## ğŸš¨ **ENTRY REQUIREMENTS**
ğŸ›‘ VALIDATE-GATE: Prerequisites
- [ ] Source directory confirmed âœ…/âŒ
- [ ] Framework acknowledgment complete âœ…/âŒ

## ğŸ›‘ **EXECUTION**

ğŸ›‘ EXECUTE-NOW: List all Python source files
```bash
find src/ -name "*.py" -type f | sort
```
ğŸ›‘ PASTE-OUTPUT: Complete file list

ğŸ“Š COUNT-AND-DOCUMENT: Python files found: [NUMBER]

## ğŸ›‘ **VALIDATION GATE**
ğŸ›‘ VALIDATE-GATE: Context Loading Complete
- [ ] All files catalogued âœ…/âŒ
- [ ] File count documented âœ…/âŒ

## ğŸ¯ **NAVIGATION**
ğŸ›‘ UPDATE-TABLE: Phase 0 â†’ Complete
ğŸ¯ NEXT-MANDATORY: Phase 1 Codebase Analysis
```

#### **Step 5: Test and Iterate**

1. Execute framework with real input
2. Measure consistency across runs
3. Identify navigation gaps
4. Add validation gates where needed
5. Refine evidence requirements
6. Iterate until 80%+ success rate

---

## ğŸ¯ **Success Metrics**

A well-designed framework should achieve:

| Metric | Target | Measurement |
|--------|--------|-------------|
| **Execution Consistency** | 80-95% | Success rate across 10+ runs |
| **Instruction Compliance** | 85-95% | % of commands executed correctly |
| **Context Efficiency** | 15-25% per phase | Token utilization measurement |
| **Navigation Drift** | <5% | % of incorrect next-step selections |
| **Evidence Completeness** | 95%+ | % of required evidence provided |
| **Quality Gate Pass** | 90%+ | % of validation gates passed first time |

### **Measurement Commands**

**Test consistency:**
```bash
for i in {1..10}; do
    run_framework_test.sh > "run_$i.log"
done
diff run_*.log  # Should be minimal differences
```

**Measure context efficiency:**
```python
import tiktoken

def measure_context(file_path):
    with open(file_path) as f:
        content = f.read()
    tokens = len(tiktoken.encoding_for_model("gpt-4").encode(content))
    max_tokens = 8192  # GPT-4 context window
    utilization = (tokens / max_tokens) * 100
    print(f"{file_path}: {tokens} tokens ({utilization:.1f}%)")
```

---

## ğŸ“š **Additional Resources**

- **Command Language Glossary**: [command-language-glossary.md](command-language-glossary.md)
- **Design Patterns Library**: [framework-design-patterns.md](framework-design-patterns.md)
- **LLM Workflow Methodology**: [LLM-WORKFLOW-ENGINEERING-METHODOLOGY.md](LLM-WORKFLOW-ENGINEERING-METHODOLOGY.md)
- **V3 Test Framework**: [code-generation/tests/v3/](code-generation/tests/v3/) (reference implementation)
- **Provider DSL Framework**: [provider-dsl-development/](provider-dsl-development/) (reference implementation)

---

## ğŸ“ **Learning Path**

1. **Read**: Command Language Glossary (understand the API)
2. **Study**: V3 Test Framework structure (see patterns in action)
3. **Practice**: Build simple 3-phase framework
4. **Apply**: Design your own framework following this guide
5. **Refine**: Measure and iterate based on success metrics
6. **Contribute**: Share patterns back to this guide

---

**Remember: You're not writing documentation, you're designing a procedural API for AI execution!**
