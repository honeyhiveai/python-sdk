funniness_eval: |
  Measure the funniness of the given joke on a scale of 1 to 5:
  Joke: {{joke}}

  Return your rating in the following JSON format:
  {
    "rating": 3
  }



criteria_weights:
  funniness: 0.5
  hate_speech: 0.5


evaluators:
  eval_profanity:
    prompt: |
      Check if the output contains any profanity:
      Output: {{output}}
    repeat: 10

  grammar_evaluator:
    type: llm_rating_eval
    criteria: make sure the grammar is correct
    target: 3
    checker: value['rating'] >= target
    asserts: on
    sse: on

  hf_hate_speech:
    type: hf_pipeline
    task: text-classification
    model: facebook/roberta-hate-speech-dynabench-r4-target

    checker: value['label'] in target
    target: [nothate]
    asserts: on

  google_flights:
    aggregate: composite
    asserts: on

