# HoneyHive Schema Ecosystem Analysis
## Comprehensive Cross-Platform Architecture Review & DSL Translation Layer Strategy

**Date**: October 1, 2025  
**Version**: 1.0  
**Status**: For Team Review  
**Authors**: Architecture Team

---

## Executive Summary

This report provides a comprehensive analysis of the HoneyHive event schema as it exists across the entire ecosystemâ€”spanning Python SDK, TypeScript SDK, backend services (TypeScript/Node.js), and database layers (ClickHouse, PostgreSQL). The analysis evaluates the current state, identifies critical gaps, and proposes a strategic path forward centered on the **Universal LLM Discovery Engine DSL as a translation layer** for mapping diverse data sources to the canonical HoneyHive schema.

### Key Findings

1. **Strong Foundation**: The 4-section HoneyHive schema (inputs/outputs/config/metadata) is production-validated and semantically consistent across platforms
2. **Translation Gap**: Current manual mapping from provider-specific formats to HoneyHive schema is fragile and doesn't scale
3. **DSL Strategic Fit**: The V4 Universal LLM Discovery Engine DSL architecture provides exactly the translation layer infrastructure needed
4. **Cross-Platform Synchronization**: Manual schema maintenance across 3 languages creates drift riskâ€”need single source of truth
5. **Go Migration Ready**: With proper DSL foundation, Go migration can proceed with confidence

### Critical Recommendations

| Priority | Recommendation | Timeline | Impact |
|----------|---------------|----------|--------|
| ğŸ”´ **P0** | Implement DSL Translation Layer | 2-3 weeks | Eliminates 90% of manual mapping code |
| ğŸ”´ **P0** | Establish Schema Source of Truth | 1 week | Prevents cross-platform drift |
| ğŸŸ¡ **P1** | Provider Isolation Architecture | 3-4 weeks | Enables parallel provider development |
| ğŸŸ¡ **P1** | ClickHouse JSON Column Migration | 2 weeks | 5-10x query performance improvement |
| ğŸŸ¢ **P2** | Go Service Migration Plan | 4-8 weeks | Foundation for 3-5x throughput gains |

---

## Table of Contents

1. [Schema Architecture Overview](#1-schema-architecture-overview)
2. [Cross-Platform Schema Comparison](#2-cross-platform-schema-comparison)
3. [The Translation Problem](#3-the-translation-problem)
4. [DSL as Translation Layer](#4-dsl-as-translation-layer)
5. [Current State Assessment](#5-current-state-assessment)
6. [Strategic Recommendations](#6-strategic-recommendations)
7. [Implementation Roadmap](#7-implementation-roadmap)
8. [Risk Analysis & Mitigation](#8-risk-analysis--mitigation)
9. [Success Metrics](#9-success-metrics)

---

## 1. Schema Architecture Overview

### 1.1 The HoneyHive Event Schema

The HoneyHive event schema serves as the **canonical data model** for all observability events across the entire platform. It provides a unified structure that accommodates diverse LLM providers while maintaining queryability and semantic clarity.

#### Core Schema Structure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              HoneyHive Canonical Event Schema               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CORE IDENTIFICATION                                         â”‚
â”‚  â€¢ event_name: string    - Descriptive name                â”‚
â”‚  â€¢ event_type: enum      - model|chain|tool|session        â”‚
â”‚  â€¢ source: string        - Origin (production/staging/etc) â”‚
â”‚  â€¢ event_id: string      - Unique identifier (UUID)        â”‚
â”‚  â€¢ session_id: string    - Session grouping (UUID)         â”‚
â”‚  â€¢ project_id: string    - Project association             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ SEMANTIC DATA (4-SECTION STRUCTURE)                        â”‚
â”‚  â€¢ inputs: Dict[str, Any]    - User inputs, prompts, etc. â”‚
â”‚  â€¢ outputs: Dict[str, Any]   - Model responses, results   â”‚
â”‚  â€¢ config: Dict[str, Any]    - Model parameters, settings â”‚
â”‚  â€¢ metadata: Dict[str, Any]  - Usage metrics, timestamps  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ RELATIONSHIPS                                               â”‚
â”‚  â€¢ parent_id: Optional[str]  - Parent event link          â”‚
â”‚  â€¢ children_ids: List[str]   - Child events               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ OBSERVABILITY                                               â”‚
â”‚  â€¢ start_time: float         - Event start (ms)           â”‚
â”‚  â€¢ end_time: float           - Event end (ms)             â”‚
â”‚  â€¢ duration: float           - Duration (ms)              â”‚
â”‚  â€¢ error: Optional[str]      - Error description          â”‚
â”‚  â€¢ metrics: Dict[str, Any]   - Computed metrics           â”‚
â”‚  â€¢ feedback: Dict[str, Any]  - User feedback              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 Ecosystem Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HONEYHIVE ECOSYSTEM                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ Python SDK   â”‚  â”‚TypeScript SDKâ”‚  â”‚   Go SDK     â”‚         â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚  (Planned)   â”‚         â”‚
â”‚  â”‚ Pydantic v2  â”‚  â”‚  Zod + TS    â”‚  â”‚  Structs     â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚         â”‚                  â”‚                  â”‚                 â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                            â”‚                                     â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚                   â”‚   HoneyHive     â”‚                           â”‚
â”‚                   â”‚  Event Schema   â”‚ â—„â”€â”€ CANONICAL             â”‚
â”‚                   â”‚  (JSON/Proto)   â”‚                           â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                            â”‚                                     â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚         â”‚                  â”‚                  â”‚                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ Ingestion    â”‚  â”‚ Enrichment   â”‚  â”‚ Evaluation   â”‚         â”‚
â”‚  â”‚  Service     â”‚  â”‚   Service    â”‚  â”‚   Service    â”‚         â”‚
â”‚  â”‚ (TypeScript) â”‚  â”‚(TypeScript)  â”‚  â”‚(TypeScript)  â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚         â”‚                  â”‚                  â”‚                 â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                            â”‚                                     â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚                   â”‚   ClickHouse    â”‚                           â”‚
â”‚                   â”‚  Events Table   â”‚                           â”‚
â”‚                   â”‚  (Optimized)    â”‚                           â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  Backend     â”‚  â”‚  Beekeeper   â”‚  â”‚   Frontend   â”‚         â”‚
â”‚  â”‚  Service     â”‚  â”‚   Service    â”‚  â”‚   Service    â”‚         â”‚
â”‚  â”‚(TypeScript)  â”‚  â”‚(TypeScript)  â”‚  â”‚(Next.js/TS)  â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. Cross-Platform Schema Comparison

### 2.1 Implementation Matrix

| Platform | Language | Validation | Core Fields | Flexible Fields | Wire Format |
|----------|----------|------------|-------------|-----------------|-------------|
| **Python SDK** | Python 3.11-3.13 | Pydantic v2 | `str`, `EventType` enum | `Dict[str, Any]` | JSON (snake_case) |
| **TypeScript SDK** | TypeScript 5.x | Zod schemas | `string`, `EventType` type | `Record<string, any>` | JSON (camelCase) |
| **Backend Services** | Node.js/TS | Zod validation | `string`, `z.string()` | `z.record(z.unknown())` | JSON (snake_case) |
| **ClickHouse DB** | SQL | Schema constraints | `String`, `LowCardinality` | `Map(String, *)` | Columnar storage |
| **Go (Planned)** | Go 1.21+ | struct tags / validator | `string`, custom types | `map[string]interface{}` | JSON / Protobuf |

### 2.2 Detailed Comparison

#### Python SDK (`schema.py`)

```python
class HoneyHiveEventSchema(BaseModel):
    """Production-validated schema from 400 real events."""
    
    # Core identification (REQUIRED)
    event_name: str
    event_type: EventType  # Enum: MODEL, CHAIN, TOOL, SESSION
    source: str
    project_id: str
    event_id: str
    session_id: str
    start_time: float
    
    # Semantic structure (FLEXIBLE)
    inputs: Dict[str, Any] = Field(default_factory=dict)
    outputs: Dict[str, Any] = Field(default_factory=dict)
    config: Dict[str, Any] = Field(default_factory=dict)
    metadata: Dict[str, Any] = Field(default_factory=dict)
    
    # Relationships
    parent_id: Optional[str] = None
    children_ids: List[str] = Field(default_factory=list)
    
    # Additional
    error: Optional[str] = None
    end_time: Optional[float] = None
    duration: Optional[float] = None
    feedback: Optional[Dict[str, Any]] = Field(default_factory=dict)
    metrics: Optional[Dict[str, Any]] = Field(default_factory=dict)
    user_properties: Optional[Dict[str, Any]] = Field(default_factory=dict)
```

**Strengths**:
- âœ… Pydantic validation ensures type safety at runtime
- âœ… Production-validated against 400 events from Deep Research
- âœ… Clear documentation and field descriptions
- âœ… Enum-based event types prevent typos

**Limitations**:
- âš ï¸ `Dict[str, Any]` loses type information for nested structures
- âš ï¸ No provider-specific type hints (OpenAI vs Anthropic vs Gemini)
- âš ï¸ Manual mapping from semantic conventions

---

#### TypeScript SDK (`event.ts`)

```typescript
export type Event = {
  // Core (snake_case in JSON, camelCase in TS)
  projectId?: string;
  source?: string;
  eventName?: string;
  eventType?: EventType;  // "session" | "model" | "tool" | "chain"
  eventId?: string;
  sessionId?: string;
  
  // Semantic structure
  config?: { [k: string]: any };
  inputs?: { [k: string]: any };
  outputs?: { [k: string]: any };
  metadata?: { [k: string]: any };
  
  // Relationships
  parentId?: string | null;
  childrenIds?: Array<string>;
  
  // Timing & observability
  startTime?: number;
  endTime?: number;
  duration?: number;
  error?: string | null;
  feedback?: { [k: string]: any };
  metrics?: { [k: string]: any };
  userProperties?: { [k: string]: any };
};

// Zod validation with automatic snake_case â†” camelCase transformation
const Event$inboundSchema = z.object({
  project_id: z.string().optional(),
  event_type: EventType$inboundSchema.optional(),
  // ... automatic remap to camelCase
}).transform((v) => remap$(v, {
  "project_id": "projectId",
  "event_type": "eventType",
  // ...
}));
```

**Strengths**:
- âœ… Zod provides runtime validation
- âœ… Automatic snake_case â†” camelCase transformation
- âœ… Speakeasy code generation ensures consistency with OpenAPI
- âœ… Type-safe JSON serialization/deserialization

**Limitations**:
- âš ï¸ All fields optional (vs Python's required core fields)
- âš ï¸ `{ [k: string]: any }` same type loss as Python
- âš ï¸ EventType values differ: `"model"` (TS) vs `MODEL` (Python enum)

---

#### Backend Services (`event_schema.js`)

```javascript
// Ingestion Service - Zod validation
const eventSchema = z.object({
  project_id: z.string(),
  session_id: uuidType,
  event_id: uuidType,
  parent_id: uuidType.optional().nullable(),
  children_ids: z.array(uuidType),
  event_type: z.string(),
  event_name: z.string(),
  
  // Semantic structure
  config: z.record(z.unknown()).nullable(),
  inputs: z.record(z.unknown()),
  outputs: singleObjectSchema.or(z.array(singleObjectSchema)),  // âš ï¸ Special case!
  
  error: z.string().optional().nullable(),
  source: z.string(),
  duration: z.number(),
  
  user_properties: z.record(z.unknown()),
  metrics: z.record(z.unknown()).nullable(),
  feedback: z.record(z.unknown()).nullable(),
  metadata: z.record(z.unknown()),
  
  tenant: z.string(),  // âš ï¸ Backend-only field
  start_time: z.number(),
  end_time: z.number(),
});
```

**Key Differences**:
- âš ï¸ **`outputs` allows arrays**: `singleObjectSchema.or(z.array(singleObjectSchema))`
- âš ï¸ **`tenant` field**: Not in SDK schemas (added by backend)
- âš ï¸ **UUID validation**: Stricter than SDK (regex pattern)
- âœ… **Required fields**: More strict than TypeScript SDK

---

#### ClickHouse Database Schema

```sql
CREATE TABLE events (
    -- Core identifiers (optimized for queries)
    "project_id" LowCardinality(FixedString(24)),
    "session_id" UUID,
    "event_id" UUID,
    "parent_id" Nullable(String),
    "children_ids" Array(String),
    "event_type" LowCardinality(String),
    "event_name" String,
    "error" LowCardinality(String) DEFAULT '0',
    "source" String,
    "duration" Float64,
    "tenant" LowCardinality(String),
    
    -- Dynamic data flattened into typed maps
    "string_maps" Map(String, String),
    "numeric_maps" Map(String, Int32),
    "float_maps" Map(String, Float64),
    "boolean_maps" Map(String, Bool),
    "array_string_maps" Map(String, Array(String)),
    "array_numeric_maps" Map(String, Array(Int32)),
    "array_float_maps" Map(String, Array(Float64)),
    "array_boolean_maps" Map(String, Array(Bool)),
    "null_values" Array(String),
    
    -- Full JSON for retrieval (fallback)
    "request_json" String,
    
    -- Timestamps
    "start_time" DateTime64(3),
    "end_time" DateTime64(3),
    "insertion_time" DateTime64(3) MATERIALIZED NOW64(),
    
    -- Materialized columns for common filters
    "evaluation_run_id" String MATERIALIZED string_maps['metadata.run_id'],
    "datapoint_id" String MATERIALIZED string_maps['metadata.datapoint_id'],
    "dataset_id" String MATERIALIZED string_maps['metadata.dataset_id']
)
ENGINE = ReplicatedReplacingMergeTree(...)
PARTITION BY toYYYYMM(start_time)
ORDER BY ("event_type", "tenant", "project_id", "event_id");
```

**Storage Strategy**:
- **Hybrid approach**: Typed maps for queries + full JSON for retrieval
- **Type-specific maps**: Separate maps for strings, numbers, floats, booleans, arrays
- **Materialized columns**: Pre-computed common query paths
- **Partitioning**: By month for time-series efficiency
- **Ordering**: Optimized for event_type + tenant + project queries

**Strengths**:
- âœ… Enables fast queries on dynamic fields without rigid schema
- âœ… Columnar storage provides excellent compression
- âœ… Materialized columns optimize common access patterns
- âœ… Full JSON fallback prevents data loss

**Limitations**:
- âš ï¸ Type information flattened during ingestion
- âš ï¸ Complex nested structures lose hierarchy in maps
- âš ï¸ Query complexity for deeply nested paths

---

### 2.3 Schema Consistency Analysis

#### âœ… **Consistent Across All Platforms**

| Field | Python | TypeScript | Backend | ClickHouse | Status |
|-------|--------|------------|---------|------------|--------|
| `event_name` | âœ… `str` | âœ… `string` | âœ… `z.string()` | âœ… `String` | **ALIGNED** |
| `event_id` | âœ… `str` | âœ… `string` | âœ… `uuidType` | âœ… `UUID` | **ALIGNED** |
| `session_id` | âœ… `str` | âœ… `string` | âœ… `uuidType` | âœ… `UUID` | **ALIGNED** |
| `project_id` | âœ… `str` | âœ… `string` | âœ… `z.string()` | âœ… `FixedString(24)` | **ALIGNED** |
| `parent_id` | âœ… `Optional[str]` | âœ… `string \| null` | âœ… `nullable()` | âœ… `Nullable(String)` | **ALIGNED** |
| `children_ids` | âœ… `List[str]` | âœ… `Array<string>` | âœ… `z.array()` | âœ… `Array(String)` | **ALIGNED** |
| `source` | âœ… `str` | âœ… `string` | âœ… `z.string()` | âœ… `String` | **ALIGNED** |
| `inputs` | âœ… `Dict[str, Any]` | âœ… `Record<string, any>` | âœ… `z.record()` | âœ… Maps | **ALIGNED** |
| `outputs` | âœ… `Dict[str, Any]` | âœ… `Record<string, any>` | âœ… `z.record()` | âœ… Maps | **ALIGNED** |
| `config` | âœ… `Dict[str, Any]` | âœ… `Record<string, any>` | âœ… `z.record()` | âœ… Maps | **ALIGNED** |
| `metadata` | âœ… `Dict[str, Any]` | âœ… `Record<string, any>` | âœ… `z.record()` | âœ… Maps | **ALIGNED** |

#### âš ï¸ **Inconsistencies & Differences**

| Field | Python | TypeScript | Backend | Issue | Impact |
|-------|--------|------------|---------|-------|--------|
| **`event_type`** | `EventType` enum<br>`MODEL, CHAIN, TOOL, SESSION` | `EventType` type<br>`"model", "tool", "chain", "session"` | `z.string()`<br>Any string | âŒ **Case mismatch**<br>âŒ **Validation differs** | **HIGH**<br>Potential data corruption |
| **`outputs`** | `Dict[str, Any]` | `Record<string, any>` | `z.record()` **OR** `z.array()` | âš ï¸ **Backend allows arrays** | **MEDIUM**<br>SDK can't handle array outputs |
| **`tenant`** | âŒ Not present | âŒ Not present | âœ… `z.string()` required | âš ï¸ **Backend-only field** | **LOW**<br>Added by ingestion service |
| **Required vs Optional** | Most fields required | All fields optional | Mix of required/optional | âš ï¸ **Validation inconsistency** | **MEDIUM**<br>Runtime errors possible |

---

## 3. The Translation Problem

### 3.1 Current State: Manual Provider Mapping

The fundamental challenge is **translating diverse provider-specific data formats** into the canonical HoneyHive schema. Currently, this happens through manual, fragile code.

#### Problem Visualization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THE TRANSLATION CHALLENGE                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Provider-Specific Formats (INPUT)                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚   OpenAI     â”‚  â”‚  Anthropic   â”‚  â”‚   Gemini     â”‚         â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚         â”‚
â”‚  â”‚ choices[]    â”‚  â”‚ content[]    â”‚  â”‚ candidates[] â”‚         â”‚
â”‚  â”‚  .message    â”‚  â”‚  .text       â”‚  â”‚  .content    â”‚         â”‚
â”‚  â”‚  .content    â”‚  â”‚  .stop_reasonâ”‚  â”‚  .parts[]    â”‚         â”‚
â”‚  â”‚  .tool_calls â”‚  â”‚  .usage      â”‚  â”‚  .finishReasonâ”‚        â”‚
â”‚  â”‚  .refusal    â”‚  â”‚              â”‚  â”‚  .safetyRatingsâ”‚       â”‚
â”‚  â”‚  .audio      â”‚  â”‚              â”‚  â”‚              â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚         â”‚                  â”‚                  â”‚                 â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                            â”‚                                     â”‚
â”‚                  âŒ MANUAL MAPPING LAYER                        â”‚
â”‚                  (Fragile, Hard to Maintain)                    â”‚
â”‚                            â”‚                                     â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚                   â”‚   HoneyHive     â”‚                           â”‚
â”‚                   â”‚  Event Schema   â”‚                           â”‚
â”‚                   â”‚                 â”‚                           â”‚
â”‚                   â”‚ inputs: {}      â”‚                           â”‚
â”‚                   â”‚ outputs: {}     â”‚                           â”‚
â”‚                   â”‚ config: {}      â”‚                           â”‚
â”‚                   â”‚ metadata: {}    â”‚                           â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Current Manual Mapping Code

#### Example: OpenAI Response â†’ HoneyHive Schema (Python SDK)

```python
# Current approach - manual, hardcoded mapping
def map_openai_to_honeyhive(openai_response):
    """Manually extract fields from OpenAI response."""
    
    # Extract from nested structure
    choices = openai_response.get("choices", [])
    first_choice = choices[0] if choices else {}
    message = first_choice.get("message", {})
    
    # Build HoneyHive event manually
    honeyhive_event = {
        "inputs": {
            # Where to get inputs? Not in response!
            "chat_history": ???  # Need to track separately
        },
        "outputs": {
            "content": message.get("content"),
            "role": message.get("role"),
            "tool_calls": message.get("tool_calls"),
            "refusal": message.get("refusal"),  # New field - code needs update!
            "audio": message.get("audio"),      # New field - code needs update!
        },
        "config": {
            "model": openai_response.get("model"),
            "temperature": ???  # Not in response, tracked elsewhere
        },
        "metadata": {
            "prompt_tokens": openai_response.get("usage", {}).get("prompt_tokens"),
            "completion_tokens": openai_response.get("usage", {}).get("completion_tokens"),
            # ... manually extract each field
        }
    }
    
    return honeyhive_event
```

**Problems**:
1. âŒ **Breaks when provider adds fields**: `refusal`, `audio` require code changes
2. âŒ **Different logic for each provider**: Anthropic has different structure
3. âŒ **No single source of truth**: Logic scattered across codebase
4. âŒ **Hard to test**: Each provider Ã— each field = combinatorial explosion
5. âŒ **No versioning**: Can't handle multiple API versions

---

### 3.3 Semantic Convention Complexity

The problem multiplies when considering **instrumentor frameworks** that wrap providers:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            INSTRUMENTOR ABSTRACTION LAYER                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Raw Provider Response                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ OpenAI: {choices: [{message: {...}}]}   â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                 â”‚                                               â”‚
â”‚         Wrapped by Instrumentor                                â”‚
â”‚                 â–¼                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ OpenInference: llm.output_messages.0.content     â”‚          â”‚
â”‚  â”‚ Traceloop:     gen_ai.completion.0.message.contentâ”‚         â”‚
â”‚  â”‚ OpenLit:       gen_ai.completion.0.message.contentâ”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                 â”‚                                               â”‚
â”‚         Each needs different extraction!                       â”‚
â”‚                 â–¼                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ HoneyHive: outputs.content               â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Current Approach**: Hardcoded extraction per instrumentor per provider

```python
# Current: Manual extraction for EACH combination
if instrumentor == "openinference":
    content = attributes.get("llm.output_messages.0.content")
elif instrumentor == "traceloop":
    content = attributes.get("gen_ai.completion.0.message.content")
elif instrumentor == "openlit":
    content = attributes.get("gen_ai.completion.0.message.content")
else:
    content = None  # Fallback
```

**Maintenance Burden**:
- 13+ providers Ã— 3+ instrumentors Ã— ~30 fields = **1,170+ mapping rules**
- Every provider update = potentially hundreds of code changes
- No systematic way to validate completeness

---

## 4. DSL as Translation Layer

### 4.1 V4 Universal LLM Discovery Engine Architecture

The V4 research provides a **declarative, provider-isolated DSL** that solves the translation problem systematically.

#### Core Concept: Configuration-Driven Translation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DSL AS TRANSLATION LAYER                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Provider Responses (Any Format)                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚   OpenAI     â”‚  â”‚  Anthropic   â”‚  â”‚   Gemini     â”‚         â”‚
â”‚  â”‚  Response    â”‚  â”‚  Response    â”‚  â”‚  Response    â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚         â”‚                  â”‚                  â”‚                 â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                            â”‚                                     â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚                  â”‚   DSL COMPILER    â”‚                          â”‚
â”‚                  â”‚                   â”‚                          â”‚
â”‚                  â”‚ âœ… YAML Configs   â”‚                          â”‚
â”‚                  â”‚ âœ… Provider Rules â”‚                          â”‚
â”‚                  â”‚ âœ… Type Safety    â”‚                          â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                            â”‚                                     â”‚
â”‚              Compiled Translation Functions                     â”‚
â”‚                            â”‚                                     â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚                   â”‚  HoneyHive      â”‚                           â”‚
â”‚                   â”‚  Event Schema   â”‚                           â”‚
â”‚                   â”‚                 â”‚                           â”‚
â”‚                   â”‚ âœ… inputs       â”‚                           â”‚
â”‚                   â”‚ âœ… outputs      â”‚                           â”‚
â”‚                   â”‚ âœ… config       â”‚                           â”‚
â”‚                   â”‚ âœ… metadata     â”‚                           â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 DSL File Structure (Provider-Isolated)

Each provider gets **4 focused YAML files** (~7KB total context):

```
config/dsl/providers/openai/
â”œâ”€â”€ structure_patterns.yaml      # Provider detection (O(1))
â”œâ”€â”€ navigation_rules.yaml        # Field extraction paths
â”œâ”€â”€ field_mappings.yaml          # â†’ HoneyHive schema mapping
â””â”€â”€ transforms.yaml              # Data transformations
```

#### 4.2.1 Structure Patterns (Provider Detection)

```yaml
# structure_patterns.yaml
version: "1.0"
provider: "openai"
dsl_type: "provider_structure_patterns"

patterns:
  openinference_openai:
    signature_fields:
      - "llm.model_name"          # Must be present
      - "llm.output_messages"     # Must be present
    model_patterns:
      - "gpt-*"                    # Model name patterns
      - "o1-*"
    confidence_weight: 0.98        # Detection confidence
    description: "OpenAI via OpenInference instrumentation"
    
  traceloop_openai:
    signature_fields:
      - "gen_ai.request.model"
      - "gen_ai.completion"
    model_patterns:
      - "gpt-*"
    confidence_weight: 0.95
    description: "OpenAI via Traceloop instrumentation"
```

**Purpose**: O(1) provider detection using frozenset pattern matching  
**Benefit**: No more `if/elif` chainsâ€”declarative pattern matching

---

#### 4.2.2 Navigation Rules (Field Extraction)

```yaml
# navigation_rules.yaml
version: "1.0"
provider: "openai"
dsl_type: "provider_navigation_rules"

navigation_rules:
  # OpenInference paths
  openinference_message_content:
    source_field: "llm.output_messages.0.content"
    extraction_method: "array_reconstruction"
    fallback_value: null
    description: "Extract message content from OpenInference"
    
  # Traceloop paths (different!)
  traceloop_message_content:
    source_field: "gen_ai.completion.0.message.content"
    extraction_method: "array_reconstruction"
    fallback_value: null
    description: "Extract message content from Traceloop"
    
  # Direct OpenAI SDK (no instrumentor)
  direct_openai_message_content:
    source_field: "choices.0.message.content"
    extraction_method: "array_reconstruction"
    fallback_value: null
    description: "Extract message content from direct OpenAI response"
```

**Purpose**: Declarative field extraction paths per instrumentor  
**Benefit**: Automatically handles all instrumentor variations

---

#### 4.2.3 Field Mappings (HoneyHive Schema)

```yaml
# field_mappings.yaml
version: "1.0"
provider: "openai"
dsl_type: "provider_field_mappings"

field_mappings:
  inputs:
    # Inputs usually come from request, not response
    chat_history:
      source_rule: "openinference_input_messages"
      required: false
      description: "Input message history"
  
  outputs:
    content:
      source_rule: "traceloop_message_content"  # Use Traceloop as primary
      required: false
      description: "Model response content"
      
    tool_calls:
      source_rule: "traceloop_tool_calls"
      required: false
      description: "Function calls made by model"
      
    refusal:
      source_rule: "traceloop_refusal"
      required: false
      description: "Model refusal message (if any)"
      
    audio:
      source_rule: "traceloop_audio"
      required: false
      description: "Audio output (if enabled)"
  
  config:
    model:
      source_rule: "traceloop_model"
      required: true
      description: "OpenAI model identifier"
      
    temperature:
      source_rule: "traceloop_temperature"
      required: false
      description: "Temperature parameter"
  
  metadata:
    prompt_tokens:
      source_rule: "traceloop_prompt_tokens"
      required: false
      description: "Input token count"
      
    completion_tokens:
      source_rule: "traceloop_completion_tokens"
      required: false
      description: "Output token count"
```

**Purpose**: Map extracted fields to HoneyHive's 4-section schema  
**Benefit**: Clear, maintainable mapping rulesâ€”no scattered code

---

#### 4.2.4 Transforms (Data Transformations)

```yaml
# transforms.yaml
version: "1.0"
provider: "openai"
dsl_type: "provider_transforms"

transforms:
  extract_user_prompt:
    function_type: "string_extraction"
    implementation: "extract_user_message_content"
    parameters:
      role_filter: "user"
      content_field: "content"
      join_multiple: true
      separator: "\n\n"
    description: "Extract user prompt from message array"
    
  extract_tool_calls:
    function_type: "array_reconstruction"
    implementation: "reconstruct_array_from_flattened"
    parameters:
      prefix: "choices.0.message.tool_calls"
      preserve_json_strings:
        - "function.arguments"  # Don't parse this JSON string!
    description: "Reconstruct tool_calls array from flattened attributes"
    
  calculate_total_tokens:
    function_type: "numeric_calculation"
    implementation: "sum_fields"
    parameters:
      source_fields: ["prompt_tokens", "completion_tokens"]
      fallback_value: 0
    description: "Calculate total token usage"
```

**Purpose**: Reusable transformation functions  
**Benefit**: Complex transformations defined once, used everywhere

---

### 4.3 Build-Time Compilation

The DSL compiles to **optimized Python structures** at build time:

```python
# Build-Time Compilation Process
# config/dsl/compiler.py

def compile_providers():
    """Compile all YAML configs to optimized bundle."""
    
    compiled_bundle = {
        # O(1) provider detection
        'provider_signatures': {
            'openai': frozenset(['llm.model_name', 'llm.output_messages']),
            'anthropic': frozenset(['llm.model_name', 'llm.content']),
            # ... all providers
        },
        
        # Pre-compiled extraction functions
        'extraction_functions': {
            'openai': compile_openai_extraction(),
            'anthropic': compile_anthropic_extraction(),
            # ... generated from YAML
        },
        
        # Field mapping tables
        'field_mappings': {
            'openai': {
                'outputs.content': 'traceloop_message_content',
                'outputs.tool_calls': 'traceloop_tool_calls',
                # ... all mappings
            }
        },
        
        # Transform registry
        'transforms': {
            'extract_user_prompt': compiled_user_prompt_fn,
            'extract_tool_calls': compiled_tool_calls_fn,
            # ... all transforms
        }
    }
    
    # Save as pickled bundle for runtime
    with open('compiled_providers.pkl', 'wb') as f:
        pickle.dump(compiled_bundle, f)
```

**Output**: Single `compiled_providers.pkl` file (~20-30KB)  
**Runtime**: O(1) operations, no YAML parsing

---

### 4.4 Runtime Processing

At runtime, the compiled bundle enables **fast, systematic translation**:

```python
# Runtime Usage
from honeyhive.tracer.dsl import UniversalSemanticProcessor

processor = UniversalSemanticProcessor()

# Span attributes from instrumentor
attributes = {
    "llm.model_name": "gpt-4o",
    "llm.output_messages.0.role": "assistant",
    "llm.output_messages.0.content": "Hello, world!",
    "llm.token_count_prompt": 10,
    "llm.token_count_completion": 3,
}

# 1. Detect provider (O(1))
provider = processor.detect_provider(attributes)  
# â†’ "openai" (frozenset match in <0.01ms)

# 2. Extract fields using compiled function
extracted = processor.extract_fields(provider, attributes)
# â†’ Uses pre-compiled openai_extraction_fn

# 3. Map to HoneyHive schema
honeyhive_event = processor.map_to_schema(extracted)
# â†’ {
#      "inputs": {...},
#      "outputs": {"content": "Hello, world!", "role": "assistant"},
#      "config": {"model": "gpt-4o"},
#      "metadata": {"prompt_tokens": 10, "completion_tokens": 3}
#    }
```

**Performance**:
- Provider detection: <0.01ms (O(1) frozenset)
- Field extraction: <0.05ms (compiled native functions)
- Schema mapping: <0.02ms (hash table lookup)
- **Total: <0.1ms per event**

---

### 4.5 DSL Advantages Over Manual Mapping

| Aspect | Manual Mapping | DSL Translation Layer |
|--------|---------------|----------------------|
| **Maintainability** | âŒ Scattered across codebase<br>âŒ Hard to find/update | âœ… Centralized YAML configs<br>âœ… Single source of truth |
| **Scalability** | âŒ O(n) if/elif chains<br>âŒ 1000+ manual rules | âœ… O(1) operations<br>âœ… Declarative rules |
| **Provider Addition** | âŒ 2-3 days (code changes)<br>âŒ High risk of bugs | âœ… 4 hours (YAML only)<br>âœ… Validation enforced |
| **Field Evolution** | âŒ Code changes everywhere<br>âŒ Easy to miss cases | âœ… Update YAML config<br>âœ… Auto-generates code |
| **Testing** | âŒ Hard to test all paths<br>âŒ No validation | âœ… Declarative = testable<br>âœ… Schema validation |
| **Multi-Language** | âŒ Rewrite for each language<br>âŒ Drift inevitable | âœ… Compile to any language<br>âœ… Consistent behavior |
| **Versioning** | âŒ No version tracking<br>âŒ Breaking changes hidden | âœ… Schema versioning built-in<br>âœ… Migration paths |

---

## 5. Current State Assessment

### 5.1 Production Validation Summary

The current HoneyHive schema has been **battle-tested in production**:

- âœ… **400 real events** analyzed from Deep Research Prod
- âœ… **4 event types** fully covered (model, chain, tool, session)
- âœ… **100% coverage** of core identification fields
- âœ… **Flexible structure** accommodates provider variations
- âœ… **Parent-child relationships** enable trace trees

**Conclusion**: The schema itself is soundâ€”the problem is **getting data into it**.

---

### 5.2 Current Mapping Implementation

#### Python SDK: Semantic Conventions System

```python
# src/honeyhive/tracer/semantic_conventions/

# Current architecture (partial DSL approach)
central_mapper.py          # Main mapping coordinator
discovery.py               # Convention detection
mapping/
â”œâ”€â”€ rule_engine.py         # Rule-based transformations
â”œâ”€â”€ rule_applier.py        # Apply rules to attributes
â”œâ”€â”€ transforms.py          # Transform functions
â””â”€â”€ patterns.py            # Pattern matching

definitions/
â”œâ”€â”€ honeyhive_v1_0_0.py   # HoneyHive native convention
â”œâ”€â”€ traceloop_v0_46_2.py  # Traceloop convention (hardcoded)
â””â”€â”€ openinference_v0_1_15.py  # OpenInference (hardcoded)
```

**Status**: Partial implementation
- âœ… Has rule engine concept
- âœ… Centralized mapping
- âŒ Definitions are hardcoded Python (not YAML)
- âŒ No provider isolation
- âŒ No build-time compilation
- âŒ Limited to 3 semantic conventions

---

#### TypeScript Backend: Validation Only

```javascript
// kubernetes/ingestion_service/app/schemas/event_schema.js

// Only validatesâ€”doesn't map!
const eventSchema = z.object({
  project_id: z.string(),
  event_type: z.string(),
  inputs: z.record(z.unknown()),
  outputs: z.record(z.unknown()),
  // ... just validation
});

// Mapping happens elsewhere (if at all)
```

**Status**: Minimal
- âœ… Zod validation works
- âŒ No systematic mapping layer
- âŒ Assumes data already in HoneyHive format
- âŒ Can't handle provider-specific formats

---

### 5.3 Gap Analysis

#### Critical Gaps

| Gap | Impact | Current Workaround | DSL Solution |
|-----|--------|-------------------|--------------|
| **Provider Detection** | HIGH | Manual if/elif chains | Frozenset signatures (O(1)) |
| **Field Evolution** | HIGH | Code changes everywhere | Update YAML, recompile |
| **Instrumentor Support** | HIGH | Hardcoded for 3 frameworks | Declarative rules per instrumentor |
| **Cross-Language Consistency** | CRITICAL | Manual sync (drift risk) | Single DSL â†’ compile to all languages |
| **Performance** | MEDIUM | O(n) detection loops | O(1) operations throughout |
| **Maintainability** | HIGH | 1000+ scattered rules | Centralized YAML configs |

---

## 6. Strategic Recommendations

### 6.1 Phase 1: DSL Translation Layer Foundation (Weeks 1-3)

#### **Objective**: Establish DSL as the canonical translation mechanism

#### Tasks:

**Week 1: DSL Infrastructure**
- [ ] Set up DSL directory structure: `config/dsl/providers/{provider}/`
- [ ] Implement YAML schema validation for DSL files
- [ ] Create DSL compiler: `config/dsl/compiler.py`
- [ ] Build runtime bundle loader with development-aware recompilation
- [ ] Write comprehensive DSL documentation

**Week 2: OpenAI DSL Implementation (Reference)**
- [ ] Create OpenAI DSL configs (4 YAML files)
- [ ] Document OpenAI semantic conventions research
- [ ] Implement and test OpenAI extraction functions
- [ ] Validate against 100 real OpenAI events
- [ ] Benchmark performance (<0.1ms requirement)

**Week 3: Provider Expansion**
- [ ] Anthropic DSL implementation
- [ ] Gemini DSL implementation
- [ ] Generate provider template script for future additions
- [ ] Cross-provider validation testing
- [ ] Performance profiling and optimization

**Success Criteria**:
- âœ… 3+ providers fully working via DSL
- âœ… <0.1ms processing time per event
- âœ… 100% accuracy vs manual mapping
- âœ… Template script enables 4-hour provider addition

---

### 6.2 Phase 2: Cross-Platform Schema Alignment (Weeks 4-6)

#### **Objective**: Single source of truth for schema across all platforms

#### Approach: JSON Schema as Source of Truth

```json
// schemas/honeyhive_event_v1.schema.json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "https://honeyhive.ai/schemas/event/v1.json",
  "title": "HoneyHive Event",
  "type": "object",
  "required": ["event_name", "event_type", "source", "event_id", "session_id", "project_id"],
  "properties": {
    "event_name": {"type": "string"},
    "event_type": {
      "type": "string",
      "enum": ["model", "chain", "tool", "session"]
    },
    "source": {"type": "string"},
    "event_id": {"type": "string", "format": "uuid"},
    "session_id": {"type": "string", "format": "uuid"},
    "project_id": {"type": "string"},
    "inputs": {"type": "object", "additionalProperties": true},
    "outputs": {"type": "object", "additionalProperties": true},
    "config": {"type": "object", "additionalProperties": true},
    "metadata": {"type": "object", "additionalProperties": true},
    // ... complete schema
  }
}
```

#### Code Generation:

**Python (Pydantic)**:
```bash
datamodel-codegen \
  --input schemas/honeyhive_event_v1.schema.json \
  --output src/honeyhive/models/event.py \
  --output-model-type pydantic_v2.BaseModel
```

**TypeScript (Zod)**:
```bash
json-schema-to-zod \
  --input schemas/honeyhive_event_v1.schema.json \
  --output src/schemas/event.ts
```

**Go (structs)**:
```bash
gojsonschema \
  --input schemas/honeyhive_event_v1.schema.json \
  --output pkg/models/event.go
```

**Tasks**:
- [ ] Create comprehensive JSON Schema for HoneyHive event
- [ ] Set up code generation pipeline (Python, TypeScript, Go)
- [ ] Integrate into CI/CD (validate on every schema change)
- [ ] Migration guide from current implementations
- [ ] Backward compatibility validation

**Success Criteria**:
- âœ… Single JSON Schema file as source of truth
- âœ… Automated generation for 3 languages
- âœ… Zero drift between language implementations
- âœ… CI/CD enforces schema validation

---

### 6.3 Phase 3: Backend Integration (Weeks 7-9)

#### **Objective**: Integrate DSL into ingestion/enrichment services

#### Architecture:

```
Ingestion Service Flow:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Receive Event (any format)                           â”‚
â”‚    â†“                                                     â”‚
â”‚ 2. DSL Detection (O(1) provider match)                  â”‚
â”‚    â†“                                                     â”‚
â”‚ 3. DSL Extraction (compiled functions)                  â”‚
â”‚    â†“                                                     â”‚
â”‚ 4. HoneyHive Schema Validation (Zod)                    â”‚
â”‚    â†“                                                     â”‚
â”‚ 5. ClickHouse Flattening (typed maps)                   â”‚
â”‚    â†“                                                     â”‚
â”‚ 6. Store in ClickHouse                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Tasks**:

**Week 7: DSL Bundle in TypeScript**
- [ ] Port DSL compiler to TypeScript (or consume Python bundle)
- [ ] Implement bundle loader for Node.js
- [ ] Create TypeScript types for compiled bundle
- [ ] Performance testing (must match Python)

**Week 8: Ingestion Service Integration**
- [ ] Replace manual mapping with DSL processor
- [ ] Implement fallback for unknown providers
- [ ] Add DSL performance metrics
- [ ] Backward compatibility mode (parallel processing)

**Week 9: Enrichment Service Integration**
- [ ] DSL-based field extraction
- [ ] Metadata enrichment via DSL transforms
- [ ] Testing across all providers
- [ ] Production rollout plan

**Success Criteria**:
- âœ… All ingestion uses DSL (no manual mapping)
- âœ… <10ms P95 latency (including DSL processing)
- âœ… 100% backward compatible with existing events
- âœ… Monitoring/alerting for DSL failures

---

### 6.4 Phase 4: Go Migration Foundation (Weeks 10-12)

#### **Objective**: Prove Go can consume DSL and match/exceed TypeScript performance

#### Strategy:

**Week 10: Go DSL Implementation**
- [ ] Implement DSL bundle loader in Go
- [ ] Parse compiled provider bundle (pickle â†’ Go structs)
- [ ] Provider detection using Go maps (O(1))
- [ ] Field extraction functions

**Week 11: Go Ingestion Service Prototype**
- [ ] Build prototype ingestion service in Go
- [ ] Benchmark vs TypeScript (target: 3-5x throughput)
- [ ] Memory profiling (<50MB per instance)
- [ ] Load testing (10K+ events/sec)

**Week 12: Production Validation**
- [ ] Shadow mode (Go processes but doesn't write)
- [ ] Validate output matches TypeScript exactly
- [ ] Performance comparison report
- [ ] Migration plan for full rollout

**Success Criteria**:
- âœ… Go achieves 3-5x throughput vs TypeScript
- âœ… Memory usage <50MB per instance
- âœ… 100% output compatibility with TypeScript
- âœ… Ready for production migration

---

## 7. Implementation Roadmap

### 7.1 12-Week Timeline

```
WEEK 1-3: DSL Translation Layer Foundation
â”œâ”€â”€ Week 1: Infrastructure (compiler, loader, validation)
â”œâ”€â”€ Week 2: OpenAI reference implementation
â””â”€â”€ Week 3: Provider expansion (Anthropic, Gemini)

WEEK 4-6: Cross-Platform Schema Alignment  
â”œâ”€â”€ Week 4: JSON Schema source of truth
â”œâ”€â”€ Week 5: Code generation pipeline (Python, TS, Go)
â””â”€â”€ Week 6: Migration and backward compatibility

WEEK 7-9: Backend Integration
â”œâ”€â”€ Week 7: DSL in TypeScript/Node.js
â”œâ”€â”€ Week 8: Ingestion service integration
â””â”€â”€ Week 9: Enrichment service integration

WEEK 10-12: Go Migration Foundation
â”œâ”€â”€ Week 10: Go DSL implementation
â”œâ”€â”€ Week 11: Go ingestion prototype
â””â”€â”€ Week 12: Production validation
```

### 7.2 Resource Requirements

| Role | Weeks 1-3 | Weeks 4-6 | Weeks 7-9 | Weeks 10-12 |
|------|-----------|-----------|-----------|-------------|
| **Python Engineer** | 100% (DSL) | 50% (validation) | 25% (support) | 0% |
| **TypeScript Engineer** | 25% (review) | 50% (code gen) | 100% (backend) | 25% (comparison) |
| **Go Engineer** | 0% | 25% (learning) | 25% (prep) | 100% (implementation) |
| **DevOps** | 10% (CI/CD) | 25% (automation) | 25% (deployment) | 50% (migration) |
| **QA/Test** | 25% (DSL testing) | 50% (cross-platform) | 75% (integration) | 100% (validation) |

**Total Effort**: ~6 engineer-months over 12 weeks

---

### 7.3 Risk Mitigation

| Risk | Probability | Impact | Mitigation |
|------|------------|--------|------------|
| **DSL Performance** | Low | High | Early benchmarking, optimization sprints |
| **Schema Drift** | Medium | High | Automated validation, CI/CD enforcement |
| **Provider Breaking Changes** | High | Medium | Version detection, graceful degradation |
| **Go Migration Issues** | Medium | High | Shadow mode testing, gradual rollout |
| **Team Capacity** | Medium | High | Phased approach, can pause between phases |

---

## 8. Risk Analysis & Mitigation

### 8.1 Technical Risks

#### Risk 1: DSL Performance Overhead

**Risk**: Translation layer adds latency to event processing

**Mitigation**:
- âœ… Build-time compilation (no runtime YAML parsing)
- âœ… O(1) provider detection (frozenset operations)
- âœ… Pre-compiled extraction functions (native Python/TS/Go)
- âœ… Performance budget: <0.1ms per event (validated in V4 research)
- âœ… Early benchmarking and optimization

**Validation Plan**:
```python
# Performance test suite
def test_dsl_performance():
    processor = UniversalSemanticProcessor()
    
    # Test with 10,000 events
    events = generate_test_events(count=10000)
    
    start = time.perf_counter()
    for event in events:
        processor.process(event)
    end = time.perf_counter()
    
    avg_latency = (end - start) / 10000
    assert avg_latency < 0.0001  # <0.1ms requirement
```

---

#### Risk 2: Schema Evolution Breaking Changes

**Risk**: Provider API changes break DSL mappings

**Mitigation**:
- âœ… Version detection in structure patterns
- âœ… Graceful fallback for unknown fields
- âœ… Multiple patterns per provider (v1, v2, etc.)
- âœ… Monitoring/alerting for unmapped fields
- âœ… Automatic issue creation for new fields

**Example**:
```yaml
# structure_patterns.yaml
patterns:
  openai_v1:
    signature_fields: ["choices.0.message.content"]
    version_range: "<=2024-12-31"
    
  openai_v2:
    signature_fields: ["choices.0.message.content", "choices.0.message.refusal"]
    version_range: ">=2025-01-01"
```

---

#### Risk 3: Cross-Language Inconsistencies

**Risk**: Generated code behaves differently across languages

**Mitigation**:
- âœ… Shared test vectors across all languages
- âœ… JSON Schema validation before code generation
- âœ… CI/CD cross-language conformance tests
- âœ… Deterministic code generation (same input â†’ same output)

**Test Strategy**:
```json
// tests/fixtures/events/openai_response.json
{
  "test_name": "openai_basic_response",
  "input": { /* raw OpenAI response */ },
  "expected_honeyhive": { /* expected output */ }
}
```

```python
# Python test
def test_openai_conformance():
    result = process_event(load_fixture("openai_response.json"))
    assert result == expected_output
```

```typescript
// TypeScript test
test("openai_conformance", () => {
  const result = processEvent(loadFixture("openai_response.json"));
  expect(result).toEqual(expectedOutput);
});
```

```go
// Go test
func TestOpenAIConformance(t *testing.T) {
    result := processEvent(loadFixture("openai_response.json"))
    assert.Equal(t, expectedOutput, result)
}
```

---

### 8.2 Organizational Risks

#### Risk 4: Team Learning Curve

**Risk**: Team unfamiliar with DSL concepts, slows adoption

**Mitigation**:
- âœ… Comprehensive documentation with examples
- âœ… "Provider Addition" guide (step-by-step)
- âœ… Automated template generation
- âœ… Pair programming sessions for first providers
- âœ… Regular knowledge sharing (weekly demos)

**Knowledge Transfer Plan**:
- Week 1: DSL architecture overview (team presentation)
- Week 2: Hands-on workshop (add a test provider)
- Week 3: Code review best practices
- Week 4: Independent provider addition (with support)

---

#### Risk 5: Migration Disruption

**Risk**: Breaking changes during migration to DSL

**Mitigation**:
- âœ… Parallel processing (old + new paths)
- âœ… Gradual rollout (% of traffic)
- âœ… Feature flags for DSL enable/disable
- âœ… Automatic rollback on errors
- âœ… Comprehensive monitoring

**Rollout Plan**:
```
Phase 1: 5% traffic (1 week)
  - Monitor for errors
  - Compare outputs (old vs new)
  
Phase 2: 25% traffic (1 week)
  - Performance validation
  - Edge case testing
  
Phase 3: 50% traffic (1 week)
  - Full load testing
  - Cost analysis
  
Phase 4: 100% traffic
  - Full migration
  - Deprecate old code
```

---

## 9. Success Metrics

### 9.1 Technical Metrics

| Metric | Baseline | Target (3 months) | Measurement |
|--------|----------|-------------------|-------------|
| **Provider Addition Time** | 2-3 days (manual code) | 4 hours (DSL config) | Time from schema analysis to production |
| **Event Processing Latency** | ~5ms (current) | <1ms (DSL) | P95 latency for translation layer |
| **Schema Drift Errors** | ~5% (manual sync) | <0.1% (automated) | Cross-platform validation failures |
| **Provider Coverage** | 3 (hardcoded) | 10+ (DSL-driven) | Number of supported providers |
| **Code Maintainability** | ~1000 LOC (scattered) | ~200 LOC + YAML | Lines of mapping code |

### 9.2 Business Metrics

| Metric | Baseline | Target | Impact |
|--------|----------|--------|--------|
| **Time to Support New Provider** | 2-3 days | 4 hours | **8x faster** go-to-market |
| **Provider API Update Response** | 1-2 days (code changes) | 1 hour (YAML update) | **16x faster** adaptation |
| **Cross-Platform Bugs** | ~5/month (drift) | <1/month (automation) | **5x reduction** in bugs |
| **Developer Onboarding Time** | 2 weeks (learn manual code) | 3 days (DSL + docs) | **3x faster** onboarding |

### 9.3 Quality Metrics

| Metric | Current | Target | Validation Method |
|--------|---------|--------|-------------------|
| **Mapping Accuracy** | ~95% (manual testing) | >99.9% (automated) | Conformance test suite |
| **Test Coverage** | ~60% (manual code) | >90% (DSL + generated) | Coverage reports |
| **Breaking Change Detection** | Manual (reactive) | Automated (proactive) | CI/CD schema validation |
| **Documentation Completeness** | ~40% (scattered) | >90% (centralized) | Doc coverage analysis |

---

## 10. Conclusion

### 10.1 Summary of Findings

The HoneyHive event schema is **fundamentally sound**â€”production-validated, semantically clear, and flexible enough to accommodate diverse providers. The core challenge is not the schema itself, but rather **translating diverse provider formats into it**.

**Current State**:
- âœ… Strong schema foundation (4-section structure)
- âœ… Production-proven across 400+ real events
- âœ… Consistent across Python, TypeScript, databases
- âŒ Manual, fragile translation layer
- âŒ Doesn't scale to 10+ providers Ã— 3+ instrumentors
- âŒ Risk of cross-platform drift

**Proposed Solution**:
The **V4 Universal LLM Discovery Engine DSL** provides exactly the translation layer infrastructure needed:
- âœ… **Declarative**: YAML configs replace scattered code
- âœ… **Systematic**: Provider isolation prevents conflicts
- âœ… **Performant**: O(1) operations, <0.1ms per event
- âœ… **Maintainable**: Single source of truth, easy updates
- âœ… **Scalable**: From 3 to 50+ providers without code bloat
- âœ… **Cross-Platform**: Compile to Python, TypeScript, Go from same DSL

### 10.2 Strategic Recommendations

**P0 - Immediate (Weeks 1-3)**:
1. âœ… Implement DSL translation layer for Python SDK
2. âœ… Establish JSON Schema as cross-platform source of truth
3. âœ… Provider isolation architecture (OpenAI, Anthropic, Gemini)

**P1 - Short-Term (Weeks 4-9)**:
4. âœ… Code generation pipeline (Python, TypeScript, Go)
5. âœ… Backend integration (ingestion/enrichment services)
6. âœ… ClickHouse query optimization (JSON columns)

**P2 - Medium-Term (Weeks 10-12)**:
7. âœ… Go migration foundation (ingestion service prototype)
8. âœ… Performance validation (3-5x throughput goals)
9. âœ… Production migration planning

### 10.3 Expected Outcomes

**3 Months**:
- 10+ providers supported via DSL (vs 3 hardcoded today)
- <1ms event processing latency (5x improvement)
- 8x faster provider addition (4 hours vs 2-3 days)
- Zero cross-platform drift (automated validation)

**6 Months**:
- Go migration complete (3-5x throughput)
- 20+ providers supported
- Self-service provider addition (template + docs)
- Full observability (provider health dashboards)

**12 Months**:
- Industry-leading provider coverage (50+)
- Sub-millisecond processing at scale (100K+ events/sec)
- Zero-maintenance translation layer (automated updates)
- Platform for next-gen observability features

---

## Appendix A: DSL Reference Implementation

### A.1 Complete OpenAI DSL Example

```yaml
# config/dsl/providers/openai/structure_patterns.yaml
version: "1.0"
provider: "openai"
dsl_type: "provider_structure_patterns"

patterns:
  openinference_openai:
    signature_fields:
      - "llm.model_name"
      - "llm.output_messages"
    model_patterns:
      - "gpt-*"
      - "o1-*"
    confidence_weight: 0.98
    
  traceloop_openai:
    signature_fields:
      - "gen_ai.request.model"
      - "gen_ai.completion"
    model_patterns:
      - "gpt-*"
    confidence_weight: 0.95

validation:
  minimum_signature_fields: 2
  maximum_patterns: 5
  confidence_threshold: 0.80
```

```yaml
# config/dsl/providers/openai/navigation_rules.yaml
version: "1.0"
provider: "openai"
dsl_type: "provider_navigation_rules"

navigation_rules:
  traceloop_message_content:
    source_field: "gen_ai.completion.0.message.content"
    extraction_method: "array_reconstruction"
    fallback_value: null
    
  traceloop_tool_calls:
    source_field: "gen_ai.completion.0.message.tool_calls"
    extraction_method: "array_reconstruction"
    fallback_value: []
    
  traceloop_prompt_tokens:
    source_field: "gen_ai.usage.prompt_tokens"
    extraction_method: "direct_copy"
    fallback_value: 0
```

```yaml
# config/dsl/providers/openai/field_mappings.yaml
version: "1.0"
provider: "openai"
dsl_type: "provider_field_mappings"

field_mappings:
  outputs:
    content:
      source_rule: "traceloop_message_content"
      required: false
      
    tool_calls:
      source_rule: "traceloop_tool_calls"
      required: false
      
  config:
    model:
      source_rule: "traceloop_model"
      required: true
      
  metadata:
    prompt_tokens:
      source_rule: "traceloop_prompt_tokens"
      required: false
```

```yaml
# config/dsl/providers/openai/transforms.yaml
version: "1.0"
provider: "openai"
dsl_type: "provider_transforms"

transforms:
  extract_tool_calls:
    function_type: "array_reconstruction"
    implementation: "reconstruct_array_from_flattened"
    parameters:
      prefix: "gen_ai.completion.0.message.tool_calls"
      preserve_json_strings:
        - "function.arguments"
```

---

## Appendix B: Glossary

**DSL (Domain-Specific Language)**: A specialized configuration language for defining provider-to-schema mappings

**Semantic Convention**: A standardized way instrumentor frameworks represent LLM data (e.g., OpenInference uses `llm.*` attributes)

**Provider Isolation**: Architecture where each provider has independent configuration files, preventing conflicts

**Build-Time Compilation**: Converting human-readable YAML configs to optimized runtime structures before deployment

**Frozenset Pattern Matching**: Using immutable sets for O(1) provider detection based on attribute signatures

**Navigation Rules**: Declarative paths for extracting fields from provider-specific structures

**Field Mappings**: Rules for mapping extracted provider data to HoneyHive's 4-section schema

**Transform Functions**: Reusable data transformation functions (e.g., array reconstruction, JSON parsing)

---

## Appendix C: Team Action Items

### For Leadership

- [ ] Review and approve 12-week implementation roadmap
- [ ] Allocate resources (6 engineer-months over 12 weeks)
- [ ] Decide on Go migration timing and priority
- [ ] Establish success metrics and review cadence

### For Engineering Team

- [ ] Review DSL architecture and provide feedback
- [ ] Identify first 5 providers to migrate to DSL
- [ ] Set up development environment for DSL work
- [ ] Schedule knowledge sharing sessions

### For DevOps

- [ ] Plan CI/CD integration for DSL compilation
- [ ] Design monitoring for DSL performance metrics
- [ ] Prepare rollout infrastructure (feature flags, etc.)

### For Product

- [ ] Prioritize provider support roadmap
- [ ] Document customer-facing improvements
- [ ] Plan communication of enhanced provider coverage

---

**Next Steps**: Schedule team review meeting to discuss findings and approve roadmap.

**Contact**: [Your Team Lead/Architect]  
**Date**: October 1, 2025

